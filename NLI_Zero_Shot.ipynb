{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b230ab55-25f7-4a96-aa38-7db322d4d02c",
   "metadata": {},
   "source": [
    "# Zero-Shot NLI Sentiment (Dutch) — LoicDL Models (Title + Lead Only)\n",
    "\n",
    "This notebook applies zero-shot text classification via NLI to Dutch news articles, using only the **title** and **lead** extracted in the same way as the baseline notebook.\n",
    "\n",
    "Tested models:\n",
    "- `LoicDL/bert-base-dutch-cased-finetuned-snli`\n",
    "- `LoicDL/robbert-v2-dutch-finetuned-snli`\n",
    "- `loicDL/robbertje-dutch-finetuned-snli`\n",
    "\n",
    "Output: per article the predicted label and per-class scores (positief/negatief/neutraal) for each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c659694-d732-4497-a0f4-5234a61594f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\semde\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] Kan opgegeven procedure niet vinden'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os, re, json\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from ftfy import fix_text as _ftfy_fix\n",
    "import unicodedata\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "OUT_CSV  = \"nli_results.csv\"\n",
    "\n",
    "# LoicDL NLI models (all three)\n",
    "NLI_MODELS = [\n",
    "    \"LoicDL/bert-base-dutch-cased-finetuned-snli\",\n",
    "    \"LoicDL/robbert-v2-dutch-finetuned-snli\",\n",
    "    \"loicDL/robbertje-dutch-finetuned-snli\",\n",
    "]\n",
    "\n",
    "# Zero-shot labels & template (binary)\n",
    "ZS_LABELS = [\"positief\", \"negatief\"]\n",
    "HYPOTHESIS_TEMPLATE = \"Het sentiment van deze tekst is {}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05a8798c-bd67-4f35-ad60-d4b89c62d15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEXIS_HEADER_PATTERNS = [\n",
    "    r\"^About LexisNexis.*\",\n",
    "    r\"^Privacy Policy.*\",\n",
    "    r\"^Terms .* Conditions.*\",\n",
    "    r\"^Copyright.*\",\n",
    "    r\"^User Name:.*\",\n",
    "    r\"^Date and Time:.*\",\n",
    "    r\"^Job Number:.*\",\n",
    "    r\"^Documents \\\\(\\\\d+\\\\).*\",\n",
    "    r\"^Client/Matter:.*\",\n",
    "    r\"^Search Terms:.*\",\n",
    "    r\"^Search Type:.*\",\n",
    "    r\"^Content Type.*\",\n",
    "    r\"^http[s]?://\\\\S+\",\n",
    "    r\"^Page \\\\d+ of \\\\d+\",\n",
    "    r\"^Load-Date:.*\",\n",
    "    r\"^End of Document\",\n",
    "    r\"^Classification$\",\n",
    "    r\"^Language:.*\",\n",
    "    r\"^Publication-Type:.*\",\n",
    "    r\"^Subject:.*\",\n",
    "    r\"^Industry:.*\",\n",
    "    r\"^\\\\s*Bookmark_\\\\d+\\\\s*$\"\n",
    "]\n",
    "\n",
    "HEADER_REGEXES = [re.compile(pat, flags=re.IGNORECASE) for pat in LEXIS_HEADER_PATTERNS]\n",
    "\n",
    "def clean_lines(lines: List[str]) -> List[str]:\n",
    "    out = []\n",
    "    for ln in lines:\n",
    "        s = ln.strip()\n",
    "        if not s:\n",
    "            out.append(\"\")\n",
    "            continue\n",
    "        if any(rx.match(s) for rx in HEADER_REGEXES):\n",
    "            continue\n",
    "        s = re.sub(r\"\\\\s+\", \" \", s)\n",
    "        out.append(s)\n",
    "    txt = \"\\\\n\".join(out)\n",
    "    txt = re.sub(r\"\\\\n{3,}\", \"\\\\n\\\\n\", txt)\n",
    "    return [ln for ln in txt.split(\"\\\\n\")]\n",
    "    \n",
    "def pdf_to_text(path: str) -> str:\n",
    "    texts = []\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            t = page.extract_text(x_tolerance=1, y_tolerance=1) or \"\"\n",
    "            if t:\n",
    "                texts.append(t)\n",
    "    raw = \"\\\\n\".join(texts)\n",
    "    raw = raw.replace(\"\\\\r\\\\n\", \"\\\\n\").replace(\"\\\\r\", \"\\\\n\")\n",
    "    lines = raw.split(\"\\\\n\")\n",
    "    lines = clean_lines(lines)\n",
    "    return \"\\\\n\".join(lines)\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    if not s:\n",
    "        return s\n",
    "    # snelle mojibake-detectie en herstel (bv. 'patiÃ«nt' → 'patiënt')\n",
    "    if re.search(r\"[ÂÃ]|Ã.|â€|â€™|â€œ|â€�|â€“|â€”\", s):\n",
    "        try:\n",
    "            s = s.encode(\"latin-1\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    # unicode normalisatie + wat typografische tekens rechtzetten\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    s = (s.replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\")\n",
    "           .replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "           .replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
    "           .replace(\"\\u00ad\", \"\"))  # soft hyphen\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b299eb29-f37c-45e5-b56b-0d73738f6183",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_MARKERS = [\n",
    "    r\"^Classification$\", r\"^End of Document$\", r\"^Load-Date:\", r\"^Subject:\", r\"^Industry:\",\n",
    "    r\"^Language:\", r\"^Publication-Type:\", r\"^Graphic$\", r\"^Bookmark_\\d+\\s*$\"\n",
    "]\n",
    "STOP_RX = re.compile(\"|\".join(STOP_MARKERS), re.IGNORECASE)\n",
    "\n",
    "def extract_title_lead_body(clean_text: str):\n",
    "    \"\"\"\n",
    "    LexisNexis-structuur:\n",
    "      [koppen/metadata] ... \n",
    "      Title (vaak 1-2x herhaald)\n",
    "      Krant / datum / sectie / lengte / byline ...\n",
    "      Body\n",
    "      <artikeltekst (meerdere alinea's, soms paginabreaks)>\n",
    "      Classification / End of Document / etc.\n",
    "\n",
    "    Output:\n",
    "      title: 1 regel\n",
    "      lead:  korte samenvatting (eerste paragraaf na Body, overslaat 1-woord labels zoals 'Column', 'Geneesmiddelen')\n",
    "      body:  rest tot aan STOP_MARKERS\n",
    "    \"\"\"\n",
    "    # 1) naar regels\n",
    "    lines = [ln.strip() for ln in clean_text.replace(\"\\r\", \"\\n\").split(\"\\n\")]\n",
    "    lines = [ln for ln in lines if ln is not None]  # behoud lege regels als scheiding\n",
    "\n",
    "    # 2) vind titelkandidaat (eerste niet-lege regel die NIET 'Page x of y' / URL / About is)\n",
    "    def is_noise_title(l):\n",
    "        if not l: return True\n",
    "        if re.match(r\"^Page \\d+ of \\d+$\", l): return True\n",
    "        if re.match(r\"^http[s]?://\", l, re.I): return True\n",
    "        if \"About LexisNexis\" in l or \"Privacy Policy\" in l or \"Terms\" in l: return True\n",
    "        return False\n",
    "\n",
    "    first_nonempty = next((i for i,l in enumerate(lines) if l and not is_noise_title(l)), None)\n",
    "    title = lines[first_nonempty] if first_nonempty is not None else \"\"\n",
    "\n",
    "    # 3) vind de EERSTE 'Body' regel (dit markeert begin van inhoud)\n",
    "    body_idx = next((i for i,l in enumerate(lines) if l.strip().lower() == \"body\"), None)\n",
    "    if body_idx is None:\n",
    "        # fallback: soms staat 'Body' met extra tekst eronder; probeer een zachtere match\n",
    "        body_idx = next((i for i,l in enumerate(lines) if re.fullmatch(r\"\\s*Body\\s*\", l, re.I)), None)\n",
    "\n",
    "    # als geen Body gevonden: alles na title als body (zeldzaam)\n",
    "    start_idx = (body_idx + 1) if body_idx is not None else ((first_nonempty + 1) if first_nonempty is not None else 0)\n",
    "\n",
    "    # 4) knip af bij eerste STOP_MARKER\n",
    "    end_idx = None\n",
    "    for j in range(start_idx, len(lines)):\n",
    "        if STOP_RX.match(lines[j] or \"\"):\n",
    "            end_idx = j\n",
    "            break\n",
    "    content_lines = lines[start_idx:end_idx] if end_idx else lines[start_idx:]\n",
    "\n",
    "    # 5) verwijder bekende tussenkopjes / 1-woord labels direct na Body (bv. 'Column', 'Geneesmiddelen')\n",
    "    while content_lines and re.fullmatch(r\"[A-Za-zÀ-ÿ\\-’'`]+\", content_lines[0]):\n",
    "        # laat staan als het duidelijk een zin is (eindigt op .?!)\n",
    "        if re.search(r\"[.!?]$\", content_lines[0]): break\n",
    "        # anders overslaan (één-woord of korte rubriek)\n",
    "        content_lines.pop(0)\n",
    "\n",
    "    # 6) maak paragrafen (lege regel = scheiding; als die ontbreken, per “lege regel” simuleren op dubbele spaties)\n",
    "    raw = \"\\n\".join(content_lines)\n",
    "    # normaliseer meerdere lege regels\n",
    "    raw = re.sub(r\"\\n{3,}\", \"\\n\\n\", raw).strip()\n",
    "    paras = [p.strip() for p in re.split(r\"\\n\\s*\\n\", raw) if p.strip()]\n",
    "    if not paras:\n",
    "        # fallback: forceer paragrafen grofweg op zinsafsluiting\n",
    "        paras = [p.strip() for p in re.split(r\"(?<=[.!?])\\s+\", raw) if p.strip()]\n",
    "\n",
    "    # 7) lead = eerste paragraaf (max 2-3 zinnen), body = rest\n",
    "    if paras:\n",
    "        # knip lead op 2-3 zinnen\n",
    "        sents = re.split(r\"(?<=[.!?])\\s+\", paras[0])\n",
    "        lead = \" \".join(sents[:3]).strip()\n",
    "        remainder = \" \".join(sents[3:]).strip()\n",
    "        body_paras = ([remainder] if remainder else []) + paras[1:]\n",
    "        body = \"\\n\\n\".join(body_paras).strip()\n",
    "    else:\n",
    "        lead, body = \"\", \"\"\n",
    "\n",
    "    title = normalize_text(title)\n",
    "    lead  = normalize_text(lead)\n",
    "    body  = normalize_text(body)\n",
    "\n",
    "    return title.strip(), lead, body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04dc1ed0-b702-4c6b-81d6-b7e66bb59634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Bericht 007 Nederlandse pati_nt wacht te lang op betere medicijnen tegen kanker.pdf ===\n",
      "TITLE: Nederlandse patiënt wacht te lang op betere medicijnen tegen kanker\n",
      "LEAD : Wat een prachtig bericht onlangs, dat meer kankerpatiënten de afgelopen decennia bleven leven. Twee derde van de patiënten met de diagnose kanker leeft na vijf jaar nog. Een vooruitgang die volgens het Integraal Kankercentrum Nederland mede te danken is aan innovatieve geneesmiddelen tegen gevorderde en uitgezaaide kanker.\n",
      "BODY len: 3697 chars\n",
      "BODY preview: Nog niet voor alle soorten, maar in ieder geval voor huid-, long-, prostaat-, bloed-, nier- en blaaskanker. Dat zijn in aantal niet de minste. Maar het is jammer dat het zo lang duurt voordat dergelijke geneesmiddelen na goedkeuring door de Amerikaanse autoriteiten in de Nederlandse praktijk terecht ...\n",
      "\n",
      "=== Bericht 010_Nieuwe kankermedicijnen leveren meer financi_le winst op dan gezondheidswinst.pdf ===\n",
      "TITLE: Nieuwe kankermedicijnen leveren meer financiële winst op dan gezondheidswinst\n",
      "LEAD : Vorige week verscheen in Trouw een artikel met de prikkelende kop: 'Kankerpatiënt krijgt te laat toegang tot nieuwe medicijnen.' De auteur van dit opiniestuk, zelf sinds 2018 longkankerpatiënt, stelt dat het veel te lang duurt voordat kankermedicijnen beschikbaar komen in ons land. 'Nederland is een mooi land waarin uiteindelijk bijna alle in de VS goedgekeurde kankergeneesmiddelen in het basispakket komen. Maar de wachttijd is te lang.\n",
      "BODY len: 4342 chars\n",
      "BODY preview: Om de kosten hoeven we de toegang tot deze middelen niet uit te stellen', zo beweert hij. Ik vermoed dat Big Pharma zich bij lezing in de handen stond te wrijven. De werkelijkheid is dat er de afgelopen jaren zeer veel innovatieve en dure kankermedicijnen op de markt zijn gekomen. Middelen die in de ...\n",
      "\n",
      "=== Bericht 011_Hoe controleer je verstopte moedervlekken_.pdf ===\n",
      "TITLE: Hoe controleer je verstopte moedervlekken?\n",
      "LEAD : Preventie het consult Meer dan twintig jaar geleden ontdekte ze op haar buik een moedervlek die van kleur veranderde. Het bleek een melanoom, die bovendien was uitgezaaid naar de lymfeklieren. Artsen hebben alle kwaadaardige cellen weggehaald en de tumor is nooit meer teruggekomen.\n",
      "BODY len: 2904 chars\n",
      "BODY preview: Toch zit de schrik er nog goed in en controleert ze elke dag haar lichaam op verdachte vlekjes. Alleen, zo vraagt deze lezer (61) zich af, hoe controleer je moedervlekken onder je haar? Eerst over die dagelijkse inspectie. Dat is echt niet nodig, zegt Soe Janssens, dermatoloog bij het Antoni van Lee ...\n",
      "\n",
      "=== Bericht 016_Ik vind het erg als _n infuus van 25.000 euro wordt weggegooid_.pdf ===\n",
      "TITLE: 'Ik vind het erg als 'n infuus van 25.000 euro wordt weggegooid'\n",
      "LEAD : Slimme ideeën van apotheker Roelof van Leeuwen besparen miljoenen Ellen van Gaalen Rotterdam Waarom schrijven artsen 1005 milligram van een oncologisch middel voor, terwijl de verpakkingen per 100 milligram gaan? ,,Dan gooien we dus 95 procent weg. Dat zijn vaak dure medicijnen.\" Eenmaal aan een patiënt gegeven middelen mogen na inlevering niet aan een andere patiënt worden gegeven.\n",
      "BODY len: 4858 chars\n",
      "BODY preview: Ziekenhuisapotheker Roelof van Leeuwen zet zich al jaren in voor doelmatiger gebruik van oncologische middelen. ,,Ik vind het erg als een infuus ter waarde van 25.000 euro wordt weggegooid\", zegt hij. Daarom is het zijn missie om zuiniger met medicijnen tegen kanker te leren omgaan. Sinds 2016 werkt ...\n"
     ]
    }
   ],
   "source": [
    "test_files = [\n",
    "    \"data/Bericht 007 Nederlandse pati_nt wacht te lang op betere medicijnen tegen kanker.pdf\",\n",
    "    \"data/Bericht 010_Nieuwe kankermedicijnen leveren meer financi_le winst op dan gezondheidswinst.pdf\",\n",
    "    \"data/Bericht 011_Hoe controleer je verstopte moedervlekken_.pdf\",\n",
    "    \"data/Bericht 016_Ik vind het erg als _n infuus van 25.000 euro wordt weggegooid_.pdf\",\n",
    "]\n",
    "\n",
    "for fp in test_files:\n",
    "    txt = pdf_to_text(fp)           # jouw bestaande pdf->text + clean\n",
    "    title, lead, body = extract_title_lead_body(txt)\n",
    "    print(\"\\n===\", os.path.basename(fp), \"===\")\n",
    "    print(\"TITLE:\", title[:120])\n",
    "    print(\"LEAD :\", lead)\n",
    "    print(\"BODY len:\", len(body), \"chars\")\n",
    "    print(\"BODY preview:\", body[:300].replace(\"\\n\",\" \") + \" ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc4190df-1f08-4e37-ab08-450e21715ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_id_from_filename(fname: str) -> int:\n",
    "    m = re.search(r\"bericht[_\\s-]*(\\d+)\", fname, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    # fallback: eerste nummer\n",
    "    m2 = re.search(r\"(\\d+)\", fname)\n",
    "    return int(m2.group(1)) if m2 else -1\n",
    "    \n",
    "def chunk_body(\n",
    "    text: str,\n",
    "    tokenizer,\n",
    "    *,\n",
    "    max_tokens: int = 400,\n",
    "    stride: int = 50,\n",
    "    prefer_paragraphs: bool = True,\n",
    "    reserve_for_hypothesis: int = 48,   # marge voor [CLS]/[SEP] + hypothesis tokens\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Hakt 'text' in token-gebaseerde chunks met overlap:\n",
    "      - Houdt rekening met model_max_length en extra tokens voor NLI.\n",
    "      - Optioneel: eerst per paragraaf splitsen (lege regel = scheiding); \n",
    "        te lange paragrafen worden verder gechunked.\n",
    "      - Geeft decoderede tekst-chunks terug (zonder special tokens).\n",
    "    \"\"\"\n",
    "    text = (text or \"\").strip()\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    # Veilig step/eff_max bepalen\n",
    "    if stride >= max_tokens:\n",
    "        raise ValueError(f\"'stride' ({stride}) must be < 'max_tokens' ({max_tokens}).\")\n",
    "\n",
    "    model_max = getattr(tokenizer, \"model_max_length\", 512)\n",
    "    if not isinstance(model_max, int) or model_max <= 0 or model_max > 4096:\n",
    "        model_max = 512  # sane default\n",
    "\n",
    "    eff_max = max(16, min(max_tokens, model_max - max(0, reserve_for_hypothesis)))\n",
    "\n",
    "    def _chunk_ids(ids: List[int]) -> List[str]:\n",
    "        if len(ids) <= eff_max:\n",
    "            # decode één keer\n",
    "            s = tokenizer.decode(ids, clean_up_tokenization_spaces=True, skip_special_tokens=True).strip()\n",
    "            return [s] if s else []\n",
    "        chunks = []\n",
    "        i = 0\n",
    "        step = max(1, eff_max - stride)\n",
    "        L = len(ids)\n",
    "        while i < L:\n",
    "            j = min(i + eff_max, L)\n",
    "            window = ids[i:j]\n",
    "            if not window:\n",
    "                break\n",
    "            s = tokenizer.decode(window, clean_up_tokenization_spaces=True, skip_special_tokens=True).strip()\n",
    "            if s:\n",
    "                chunks.append(s)\n",
    "            if j >= L:\n",
    "                break\n",
    "            i += step\n",
    "        return chunks\n",
    "\n",
    "    out: List[str] = []\n",
    "    if prefer_paragraphs:\n",
    "        # echte newlines gebruiken; lege regel = paragraafscheiding\n",
    "        paras = [p.strip() for p in re.split(r\"\\n\\s*\\n\", text) if p.strip()]\n",
    "        for p in paras:\n",
    "            ids = tokenizer.encode(p, add_special_tokens=False)\n",
    "            out.extend(_chunk_ids(ids))\n",
    "    else:\n",
    "        ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "        out.extend(_chunk_ids(ids))\n",
    "\n",
    "    # filter eventuele lege/duplicaten door whitespace\n",
    "    return [c for c in out if c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e01668f0-9fbd-42ce-b0b8-3fccb933ff8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pipeline for: LoicDL/bert-base-dutch-cased-finetuned-snli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pipeline for: LoicDL/robbert-v2-dutch-finetuned-snli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pipeline for: loicDL/robbertje-dutch-finetuned-snli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipelines ready.\n"
     ]
    }
   ],
   "source": [
    "pipelines = {}\n",
    "for mid in NLI_MODELS:\n",
    "    print(f\"Loading pipeline for: {mid}\")\n",
    "    pipelines[mid] = pipeline(\n",
    "        task=\"zero-shot-classification\",\n",
    "        model=mid\n",
    "    )\n",
    "print(\"Pipelines ready.\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(NLI_MODELS[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46f6d6a4-a33b-46c6-991c-c5ea7d82467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nli_score_text_bin_single(text: str, *, clf):\n",
    "    \"\"\"\n",
    "    Binaire scores (p_pos, p_neg) met JOUW enkele template en labels.\n",
    "    - gebruikt exact HYPOTHESIS_TEMPLATE en ZS_LABELS\n",
    "    - case-insensitive mapping\n",
    "    - normaliseert over POS/NEG\n",
    "    \"\"\"\n",
    "    txt = (text or \"\").strip()\n",
    "    if not txt:\n",
    "        return 0.5, 0.5\n",
    "\n",
    "    out = clf(\n",
    "        txt,\n",
    "        ZS_LABELS,\n",
    "        multi_label=False,\n",
    "        hypothesis_template=HYPOTHESIS_TEMPLATE\n",
    "    )\n",
    "    labels = [str(x).strip().lower() for x in out[\"labels\"]]\n",
    "    scores = [float(x) for x in out[\"scores\"]]\n",
    "    smap = dict(zip(labels, scores))\n",
    "\n",
    "    p_pos = smap.get(\"positief\", 0.0)\n",
    "    p_neg = smap.get(\"negatief\", 0.0)\n",
    "\n",
    "    s = p_pos + p_neg\n",
    "    if s > 0:\n",
    "        p_pos /= s\n",
    "        p_neg /= s\n",
    "    else:\n",
    "        p_pos = p_neg = 0.5\n",
    "    return p_pos, p_neg\n",
    "\n",
    "\n",
    "def classify_title_lead_body_bin(\n",
    "    title: str,\n",
    "    lead: str,\n",
    "    body: str,\n",
    "    *,\n",
    "    clf,\n",
    "    tokenizer,\n",
    "    max_tokens: int = 400,\n",
    "    stride: int = 50,\n",
    "    max_body_chunks: int | None = None,\n",
    "    debug: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Binair (positief/negatief) op basis van TITLE + LEAD + BODY-CHUNKS.\n",
    "    - Elke part (title, lead, elke body-chunk) telt 1x mee (gelijke weging).\n",
    "    - chunk_body(...) komt uit jouw notebook.\n",
    "    \"\"\"\n",
    "    parts: list[tuple[str, str]] = []\n",
    "\n",
    "    t = (title or \"\").strip()\n",
    "    l = (lead  or \"\").strip()\n",
    "    b = (body  or \"\").strip()\n",
    "\n",
    "    if t: parts.append((\"title\", t))\n",
    "    if l: parts.append((\"lead\",  l))\n",
    "\n",
    "    # Body → token-gebaseerde chunks via jouw bestaande utility\n",
    "    if b:\n",
    "        body_chunks = chunk_body(b, tokenizer, max_tokens=max_tokens, stride=stride)\n",
    "        if max_body_chunks is not None and len(body_chunks) > max_body_chunks:\n",
    "            body_chunks = body_chunks[:max_body_chunks]\n",
    "        for i, ch in enumerate(body_chunks):\n",
    "            ch = (ch or \"\").strip()\n",
    "            if ch:\n",
    "                parts.append((f\"body[{i}]\", ch))\n",
    "\n",
    "    if not parts:\n",
    "        if debug: print(\"[DEBUG] No text found (title/lead/body empty).\")\n",
    "        return \"negatief\", {\"positief\": 0.5, \"negatief\": 0.5}\n",
    "\n",
    "    pos_sum = 0.0\n",
    "    neg_sum = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for name, txt in parts:\n",
    "        p_pos, p_neg = nli_score_text_bin_single(txt, clf=clf)\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] part={name:8s} len={len(txt):4d} -> p_pos={p_pos:.4f} p_neg={p_neg:.4f}\")\n",
    "        pos_sum += p_pos\n",
    "        neg_sum += p_neg\n",
    "        n += 1\n",
    "\n",
    "    # simpele gemiddelde (gelijke weging)\n",
    "    p_pos_final = pos_sum / n\n",
    "    p_neg_final = neg_sum / n\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] AGG -> p_pos={p_pos_final:.4f} p_neg={p_neg_final:.4f} (n={n})\")\n",
    "\n",
    "    label = \"positief\" if p_pos_final >= p_neg_final else \"negatief\"\n",
    "    return label, {\"positief\": float(p_pos_final), \"negatief\": float(p_neg_final)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45fc9644-be74-424a-9390-2e6f5de90732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TITLE + LEAD + BODY CHUNKS (per model, met extractie in de loop) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (759 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Model] LoicDL/bert-base-dutch-cased-finetuned-snli\n",
      "[EXTRACT] TITLE: Nederlandse patiënt wacht te lang op betere medicijnen tegen kanker\n",
      "[EXTRACT] LEAD : Wat een prachtig bericht onlangs, dat meer kankerpatiënten de afgelopen decennia bleven leven. Twee derde van de patiënten met de diagnose kanker leeft na vijf jaar nog. Een vooruitgang die volgens het Integraal Kankerce…\n",
      "[EXTRACT] BODY chars: 3697\n",
      "[DEBUG] part=title    len=  67 -> p_pos=0.6052 p_neg=0.3948\n",
      "[DEBUG] part=lead     len= 324 -> p_pos=0.7392 p_neg=0.2608\n",
      "[DEBUG] part=body[0]  len=1999 -> p_pos=0.5361 p_neg=0.4639\n",
      "[DEBUG] part=body[1]  len=1922 -> p_pos=0.4937 p_neg=0.5063\n",
      "[DEBUG] part=body[2]  len= 288 -> p_pos=0.5539 p_neg=0.4461\n",
      "[DEBUG] AGG -> p_pos=0.5856 p_neg=0.4144 (n=5)\n",
      "[RESULT] ('positief', {'positief': 0.5856149931827919, 'negatief': 0.4143850068172081})\n",
      "\n",
      "[Model] LoicDL/robbert-v2-dutch-finetuned-snli\n",
      "[EXTRACT] TITLE: Nederlandse patiënt wacht te lang op betere medicijnen tegen kanker\n",
      "[EXTRACT] LEAD : Wat een prachtig bericht onlangs, dat meer kankerpatiënten de afgelopen decennia bleven leven. Twee derde van de patiënten met de diagnose kanker leeft na vijf jaar nog. Een vooruitgang die volgens het Integraal Kankerce…\n",
      "[EXTRACT] BODY chars: 3697\n",
      "[DEBUG] part=title    len=  67 -> p_pos=0.6773 p_neg=0.3227\n",
      "[DEBUG] part=lead     len= 324 -> p_pos=0.9730 p_neg=0.0270\n",
      "[DEBUG] part=body[0]  len=1999 -> p_pos=0.5432 p_neg=0.4568\n",
      "[DEBUG] part=body[1]  len=1922 -> p_pos=0.5335 p_neg=0.4665\n",
      "[DEBUG] part=body[2]  len= 288 -> p_pos=0.5496 p_neg=0.4504\n",
      "[DEBUG] AGG -> p_pos=0.6554 p_neg=0.3446 (n=5)\n",
      "[RESULT] ('positief', {'positief': 0.6553510430582102, 'negatief': 0.34464895694178976})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Model] loicDL/robbertje-dutch-finetuned-snli\n",
      "[EXTRACT] TITLE: Nederlandse patiënt wacht te lang op betere medicijnen tegen kanker\n",
      "[EXTRACT] LEAD : Wat een prachtig bericht onlangs, dat meer kankerpatiënten de afgelopen decennia bleven leven. Twee derde van de patiënten met de diagnose kanker leeft na vijf jaar nog. Een vooruitgang die volgens het Integraal Kankerce…\n",
      "[EXTRACT] BODY chars: 3697\n",
      "[DEBUG] part=title    len=  67 -> p_pos=0.3062 p_neg=0.6938\n",
      "[DEBUG] part=lead     len= 324 -> p_pos=0.5068 p_neg=0.4932\n",
      "[DEBUG] part=body[0]  len=1999 -> p_pos=0.5007 p_neg=0.4993\n",
      "[DEBUG] part=body[1]  len=1922 -> p_pos=0.5021 p_neg=0.4979\n",
      "[DEBUG] part=body[2]  len= 288 -> p_pos=0.5078 p_neg=0.4922\n",
      "[DEBUG] AGG -> p_pos=0.4647 p_neg=0.5353 (n=5)\n",
      "[RESULT] ('negatief', {'positief': 0.464725296072167, 'negatief': 0.5352747039278329})\n"
     ]
    }
   ],
   "source": [
    "PDF_PATH = \"data/Bericht 007 Nederlandse pati_nt wacht te lang op betere medicijnen tegen kanker.pdf\"\n",
    "\n",
    "print(\"\\n=== TITLE + LEAD + BODY CHUNKS (per model, met extractie in de loop) ===\")\n",
    "for mid in NLI_MODELS:\n",
    "    # 1) PDF -> text\n",
    "    clean_txt = pdf_to_text(PDF_PATH)\n",
    "\n",
    "    # 2) Extract title / lead / body\n",
    "    title, lead, body = extract_title_lead_body(clean_txt)\n",
    "\n",
    "    # (optioneel) snelle zichtcheck\n",
    "    print(f\"\\n[Model] {mid}\")\n",
    "    print(\"[EXTRACT] TITLE:\", title)\n",
    "    print(\"[EXTRACT] LEAD :\", (lead[:220] + \"…\") if len(lead) > 220 else lead)\n",
    "    print(\"[EXTRACT] BODY chars:\", len(body))\n",
    "\n",
    "    # 3) Classify (title + lead + body-chunks), gelijke weging, debug aan\n",
    "    result = classify_title_lead_body_bin(\n",
    "        title, lead, body,\n",
    "        clf=pipelines[mid],\n",
    "        tokenizer=tokenizer,   # jouw bestaande tokenizer (voor chunk_body)\n",
    "        max_tokens=400,\n",
    "        stride=50,\n",
    "        # max_body_chunks=12,  # (optioneel) limiter om ruis/runtime te beperken\n",
    "        debug=True\n",
    "    )\n",
    "    print(\"[RESULT]\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3f58865-80d8-4012-a43a-c0b634a70408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|█████████████████████████████████████████████████████████████████| 70/70 [01:53<00:00,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                               file  \\\n",
      "0   7  Bericht 007 Nederlandse pati_nt wacht te lang ...   \n",
      "1  10  Bericht 010_Nieuwe kankermedicijnen leveren me...   \n",
      "2  11  Bericht 011_Hoe controleer je verstopte moeder...   \n",
      "3  16  Bericht 016_Ik vind het erg als _n infuus van ...   \n",
      "4  21  Bericht 021_Wachtlijsten en personeelstekort_ ...   \n",
      "\n",
      "                                               title  \\\n",
      "0  Nederlandse patiënt wacht te lang op betere me...   \n",
      "1  Nieuwe kankermedicijnen leveren meer financiël...   \n",
      "2         Hoe controleer je verstopte moedervlekken?   \n",
      "3  'Ik vind het erg als 'n infuus van 25.000 euro...   \n",
      "4  Wachtlijsten en personeelstekort: het 'zorginf...   \n",
      "\n",
      "                                                lead  \\\n",
      "0  Wat een prachtig bericht onlangs, dat meer kan...   \n",
      "1  Vorige week verscheen in Trouw een artikel met...   \n",
      "2  Preventie het consult\\nMeer dan twintig jaar g...   \n",
      "3  Slimme ideeën van apotheker Roelof van Leeuwen...   \n",
      "4  Onbetaalbare zorg\\nDe gezondheidszorg kan het ...   \n",
      "\n",
      "  bert_base_dutch_cased_finetuned_snli_label  \\\n",
      "0                                   positief   \n",
      "1                                   positief   \n",
      "2                                   positief   \n",
      "3                                   positief   \n",
      "4                                   positief   \n",
      "\n",
      "   bert_base_dutch_cased_finetuned_snli_positief  \\\n",
      "0                                       0.672204   \n",
      "1                                       0.601400   \n",
      "2                                       0.627550   \n",
      "3                                       0.594343   \n",
      "4                                       0.563527   \n",
      "\n",
      "   bert_base_dutch_cased_finetuned_snli_negatief  \\\n",
      "0                                       0.327796   \n",
      "1                                       0.398600   \n",
      "2                                       0.372450   \n",
      "3                                       0.405657   \n",
      "4                                       0.436473   \n",
      "\n",
      "  robbert_v2_dutch_finetuned_snli_label  \\\n",
      "0                              positief   \n",
      "1                              positief   \n",
      "2                              positief   \n",
      "3                              positief   \n",
      "4                              negatief   \n",
      "\n",
      "   robbert_v2_dutch_finetuned_snli_positief  \\\n",
      "0                                  0.813066   \n",
      "1                                  0.600150   \n",
      "2                                  0.540558   \n",
      "3                                  0.650603   \n",
      "4                                  0.493724   \n",
      "\n",
      "   robbert_v2_dutch_finetuned_snli_negatief  \\\n",
      "0                                  0.186934   \n",
      "1                                  0.399850   \n",
      "2                                  0.459442   \n",
      "3                                  0.349397   \n",
      "4                                  0.506276   \n",
      "\n",
      "  robbertje_dutch_finetuned_snli_label  \\\n",
      "0                             negatief   \n",
      "1                             negatief   \n",
      "2                             negatief   \n",
      "3                             negatief   \n",
      "4                             negatief   \n",
      "\n",
      "   robbertje_dutch_finetuned_snli_positief  \\\n",
      "0                                 0.405960   \n",
      "1                                 0.490422   \n",
      "2                                 0.336561   \n",
      "3                                 0.413449   \n",
      "4                                 0.414379   \n",
      "\n",
      "   robbertje_dutch_finetuned_snli_negatief  \n",
      "0                                 0.594040  \n",
      "1                                 0.509578  \n",
      "2                                 0.663439  \n",
      "3                                 0.586551  \n",
      "4                                 0.585621  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "pdf_files = [f for f in sorted(os.listdir(DATA_DIR)) if f.lower().endswith('.pdf')]\n",
    "\n",
    "if not pdf_files:\n",
    "    print(f\"[INFO] No PDF files found in '{DATA_DIR}'.\")\n",
    "else:\n",
    "    for fname in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "        fpath = os.path.join(DATA_DIR, fname)\n",
    "        clean_txt = pdf_to_text(fpath)\n",
    "        title, lead, body = extract_title_lead_body(clean_txt)  # body NIET meer gebruiken\n",
    "\n",
    "        rec = {\n",
    "            \"id\": extract_id_from_filename(fname),\n",
    "            \"file\": fname,\n",
    "            \"title\": title,\n",
    "            \"lead\": lead,\n",
    "        }\n",
    "\n",
    "        for mid, clf in pipelines.items():\n",
    "            # Alleen title + lead (binaire NLI, geen body)\n",
    "            label, scores = classify_title_lead_body_bin(\n",
    "                title, lead, body,\n",
    "                clf=pipelines[mid],\n",
    "                tokenizer=tokenizer,          # jouw bestaande tokenizer\n",
    "                max_tokens=400,\n",
    "                stride=50,\n",
    "                # max_body_chunks=12,         # optioneel: limiet om runtime te beheersen\n",
    "                debug=False\n",
    "            )\n",
    "\n",
    "            key = mid.split(\"/\")[-1].replace(\"-\", \"_\")\n",
    "            rec[f\"{key}_label\"]     = label\n",
    "            rec[f\"{key}_positief\"]  = scores[\"positief\"]\n",
    "            rec[f\"{key}_negatief\"]  = scores[\"negatief\"]\n",
    "        rows.append(rec)\n",
    "\n",
    "df_out = pd.DataFrame(rows).sort_values(by=[\"id\", \"file\"])\n",
    "print(df_out.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be1454c1-0817-40d7-a44d-8cb2c64df1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Wrote binary results to nli_results.csv\n"
     ]
    }
   ],
   "source": [
    "df_out.to_csv(OUT_CSV, index=False)\n",
    "print(\"[DONE] Wrote binary results to nli_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dbde03-4a9e-46be-8771-9c9a1ad4c2b9",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2feae59-6656-42bd-b7cf-59319c521102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  bert_base_dutch_cased_finetuned_snli_positief  \\\n",
      "0   7                                           True   \n",
      "1  10                                           True   \n",
      "2  11                                           True   \n",
      "3  16                                           True   \n",
      "4  21                                           True   \n",
      "\n",
      "   robbert_v2_dutch_finetuned_snli_negatief  \\\n",
      "0                                     False   \n",
      "1                                     False   \n",
      "2                                     False   \n",
      "3                                     False   \n",
      "4                                      True   \n",
      "\n",
      "   robbert_v2_dutch_finetuned_snli_positief  \\\n",
      "0                                      True   \n",
      "1                                      True   \n",
      "2                                      True   \n",
      "3                                      True   \n",
      "4                                     False   \n",
      "\n",
      "   robbertje_dutch_finetuned_snli_negatief  \\\n",
      "0                                     True   \n",
      "1                                     True   \n",
      "2                                     True   \n",
      "3                                     True   \n",
      "4                                     True   \n",
      "\n",
      "   robbertje_dutch_finetuned_snli_positief  \n",
      "0                                    False  \n",
      "1                                    False  \n",
      "2                                    False  \n",
      "3                                    False  \n",
      "4                                    False  \n",
      "\n",
      "Samenvatting aantallen per model:\n",
      "                                      positief  negatief\n",
      "Model                                                   \n",
      "bert_base_dutch_cased_finetuned_snli        70         0\n",
      "robbert_v2_dutch_finetuned_snli             67         3\n",
      "robbertje_dutch_finetuned_snli              10        60\n"
     ]
    }
   ],
   "source": [
    "# === Load results (NLI models) ===\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "DF_PATH = \"nli_results.csv\"   # pas aan indien anders\n",
    "df = pd.read_csv(DF_PATH)\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Parse ID (robust)\n",
    "# ---------------------------\n",
    "def extract_id(val):\n",
    "    # Als al numeriek:\n",
    "    try:\n",
    "        return int(val)\n",
    "    except Exception:\n",
    "        pass\n",
    "    s = str(val)\n",
    "    m = re.search(r'bericht[_\\s-]*(\\d+)', s, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    # fallback: neem eerste getal dat voorkomt\n",
    "    m2 = re.search(r'(\\d+)', s)\n",
    "    return int(m2.group(1)) if m2 else None\n",
    "\n",
    "if 'id' in df.columns:\n",
    "    df['id'] = df['id'].apply(extract_id)\n",
    "elif 'file' in df.columns:\n",
    "    df['id'] = df['file'].apply(extract_id)\n",
    "else:\n",
    "    df['id'] = range(1, len(df) + 1)\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Vind alle model-labelkolommen (eindigen op \"_label\")\n",
    "# ---------------------------\n",
    "label_cols = [c for c in df.columns if c.endswith(\"_label\")]\n",
    "if not label_cols:\n",
    "    raise KeyError(\"Geen *_label kolommen gevonden in nli_results.csv\")\n",
    "\n",
    "# labels normaliseren (lowercase)\n",
    "for c in label_cols:\n",
    "    df[c] = df[c].astype(str).str.strip().str.lower()\n",
    "\n",
    "# ---------------------------\n",
    "# 3) One-hot per model\n",
    "#    -> bijv. robbert_v2_dutch_finetuned_snli_positief / _neutraal / _negatief\n",
    "# ---------------------------\n",
    "one_hot_frames = []\n",
    "for c in label_cols:\n",
    "    model_prefix = c.replace(\"_label\", \"\")\n",
    "    oh = pd.get_dummies(df[[c]].rename(columns={c: model_prefix}),\n",
    "                        columns=[model_prefix], prefix=model_prefix)\n",
    "    one_hot_frames.append(oh)\n",
    "\n",
    "df_onehot = pd.concat([df[['id']]] + one_hot_frames, axis=1)\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Bekijken (eerste regels + samenvatting per model)\n",
    "# ---------------------------\n",
    "print(df_onehot.head())\n",
    "\n",
    "# Samenvatting aantal labels per model\n",
    "summary = {}\n",
    "for c in label_cols:\n",
    "    model_prefix = c.replace(\"_label\", \"\")\n",
    "    pos = df_onehot.filter(like=f\"{model_prefix}_positief\").sum().sum()\n",
    "    neg = df_onehot.filter(like=f\"{model_prefix}_negatief\").sum().sum()\n",
    "    summary[model_prefix] = {\"positief\": int(pos), \"negatief\": int(neg)}\n",
    "\n",
    "summary_df = pd.DataFrame(summary).T\n",
    "summary_df.index.name = \"Model\"\n",
    "print(\"\\nSamenvatting aantallen per model:\")\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9bc90f3b-a7f9-4202-9488-dfa421de363b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== bert_base_dutch_cased_finetuned_snli (Zero-Shot NLI, binary) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positief      0.631     1.000     0.774        41\n",
      "    negatief      0.000     0.000     0.000        24\n",
      "\n",
      "    accuracy                          0.631        65\n",
      "   macro avg      0.315     0.500     0.387        65\n",
      "weighted avg      0.398     0.631     0.488        65\n",
      "\n",
      "Confusion matrix:\n",
      "               Pred positief  Pred negatief\n",
      "True positief             41              0\n",
      "True negatief             24              0\n",
      "\n",
      "=== robbert_v2_dutch_finetuned_snli (Zero-Shot NLI, binary) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positief      0.619     0.951     0.750        41\n",
      "    negatief      0.000     0.000     0.000        24\n",
      "\n",
      "    accuracy                          0.600        65\n",
      "   macro avg      0.310     0.476     0.375        65\n",
      "weighted avg      0.390     0.600     0.473        65\n",
      "\n",
      "Confusion matrix:\n",
      "               Pred positief  Pred negatief\n",
      "True positief             39              2\n",
      "True negatief             24              0\n",
      "\n",
      "=== robbertje_dutch_finetuned_snli (Zero-Shot NLI, binary) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positief      0.667     0.146     0.240        41\n",
      "    negatief      0.375     0.875     0.525        24\n",
      "\n",
      "    accuracy                          0.415        65\n",
      "   macro avg      0.521     0.511     0.383        65\n",
      "weighted avg      0.559     0.415     0.345        65\n",
      "\n",
      "Confusion matrix:\n",
      "               Pred positief  Pred negatief\n",
      "True positief              6             35\n",
      "True negatief              3             21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\semde\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\semde\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\semde\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# === Evaluatie: Human vs NLI (binair: positief/negatief) ===\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "DF_PATH = \"nli_results.csv\"   # ← let op: binaire output\n",
    "\n",
    "# 1) Load data\n",
    "df_h = pd.read_excel(\"Human_Sentiment.xlsx\")  # kolommen: Artikel, Sentiment\n",
    "df_n = pd.read_csv(DF_PATH)\n",
    "\n",
    "# 2) ID helpers\n",
    "def extract_id(s):\n",
    "    s = str(s)\n",
    "    m = re.search(r'bericht[_\\s-]*(\\d+)', s, flags=re.IGNORECASE)\n",
    "    if m: return int(m.group(1))\n",
    "    m2 = re.search(r'(\\d+)', s)\n",
    "    return int(m2.group(1)) if m2 else None\n",
    "\n",
    "if 'id' in df_n.columns:\n",
    "    df_n['id'] = df_n['id'].apply(extract_id)\n",
    "elif 'file' in df_n.columns:\n",
    "    df_n['id'] = df_n['file'].apply(extract_id)\n",
    "else:\n",
    "    df_n['id'] = range(1, len(df_n) + 1)\n",
    "\n",
    "if not pd.api.types.is_integer_dtype(df_h['Artikel']):\n",
    "    try:\n",
    "        df_h['Artikel'] = df_h['Artikel'].astype(int)\n",
    "    except Exception:\n",
    "        df_h['Artikel'] = df_h['Artikel'].apply(extract_id)\n",
    "\n",
    "# 3) Human labels -> alleen pos/neg\n",
    "df_h['Human_Label'] = df_h['Sentiment'].astype(str).str.strip().str.lower()\n",
    "df_h = df_h[df_h['Human_Label'].isin({'positief','negatief'})].copy()\n",
    "\n",
    "# 4) Merge\n",
    "dfm = pd.merge(df_h, df_n, left_on='Artikel', right_on='id', how='inner')\n",
    "\n",
    "# 5) Vind alle *_label kolommen (per model)\n",
    "model_label_cols = [c for c in dfm.columns if c.endswith(\"_label\")]\n",
    "if not model_label_cols:\n",
    "    raise KeyError(\"Geen *_label kolommen gevonden in NLI-resultaten.\")\n",
    "\n",
    "labels_order = ['positief','negatief']\n",
    "\n",
    "# 6) Evaluatie per model\n",
    "for col in model_label_cols:\n",
    "    model_name = col.replace(\"_label\",\"\")\n",
    "    y_true = dfm['Human_Label'].astype(str).str.lower()\n",
    "    y_pred = dfm[col].astype(str).str.lower()\n",
    "\n",
    "    mask = y_true.isin(labels_order) & y_pred.isin(labels_order)\n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "\n",
    "    print(f\"\\n=== {model_name} (Zero-Shot NLI, binary) ===\")\n",
    "    print(classification_report(\n",
    "        y_true, y_pred,\n",
    "        labels=labels_order,\n",
    "        target_names=labels_order,\n",
    "        digits=3\n",
    "    ))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels_order)\n",
    "    cm_df = pd.DataFrame(\n",
    "        cm,\n",
    "        index=[f\"True {l}\" for l in labels_order],\n",
    "        columns=[f\"Pred {l}\" for l in labels_order]\n",
    "    )\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(cm_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a490213f-575f-4b96-8d80-9662921d9dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
