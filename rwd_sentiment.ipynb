{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e94fd5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pdfplumber\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "ROB_BERT_SENTIMENT = \"DTAI-KULeuven/robbert-v2-dutch-sentiment\"\n",
    "BERTJE_SENTIMENT   = \"DTAI-KULeuven/robbertje-merged-dutch-sentiment\"\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "OUT_CSV  = \"sentiment_results.csv\"\n",
    "\n",
    "@dataclass\n",
    "class Weights:\n",
    "    title: float = 3.0\n",
    "    lead: float  = 2.0\n",
    "    body: float  = 1.0\n",
    "WEIGHTS = Weights()\n",
    "\n",
    "NEUTRAL_BAND = 0.10\n",
    "MAX_TOKENS = 400\n",
    "STRIDE     = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f202cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEXIS_HEADER_PATTERNS = [\n",
    "    r\"^About LexisNexis.*\",\n",
    "    r\"^Privacy Policy.*\",\n",
    "    r\"^Terms .* Conditions.*\",\n",
    "    r\"^Copyright.*\",\n",
    "    r\"^User Name:.*\",\n",
    "    r\"^Date and Time:.*\",\n",
    "    r\"^Job Number:.*\",\n",
    "    r\"^Documents \\\\(\\\\d+\\\\).*\",\n",
    "    r\"^Client/Matter:.*\",\n",
    "    r\"^Search Terms:.*\",\n",
    "    r\"^Search Type:.*\",\n",
    "    r\"^Content Type.*\",\n",
    "    r\"^http[s]?://\\\\S+\",\n",
    "    r\"^Page \\\\d+ of \\\\d+\",\n",
    "    r\"^Load-Date:.*\",\n",
    "    r\"^End of Document\",\n",
    "    r\"^Classification$\",\n",
    "    r\"^Language:.*\",\n",
    "    r\"^Publication-Type:.*\",\n",
    "    r\"^Subject:.*\",\n",
    "    r\"^Industry:.*\",\n",
    "    r\"^\\\\s*Bookmark_\\\\d+\\\\s*$\"\n",
    "]\n",
    "\n",
    "HEADER_REGEXES = [re.compile(pat, flags=re.IGNORECASE) for pat in LEXIS_HEADER_PATTERNS]\n",
    "\n",
    "def clean_lines(lines: List[str]) -> List[str]:\n",
    "    out = []\n",
    "    for ln in lines:\n",
    "        s = ln.strip()\n",
    "        if not s:\n",
    "            out.append(\"\")\n",
    "            continue\n",
    "        if any(rx.match(s) for rx in HEADER_REGEXES):\n",
    "            continue\n",
    "        s = re.sub(r\"\\\\s+\", \" \", s)\n",
    "        out.append(s)\n",
    "    txt = \"\\\\n\".join(out)\n",
    "    txt = re.sub(r\"\\\\n{3,}\", \"\\\\n\\\\n\", txt)\n",
    "    return [ln for ln in txt.split(\"\\\\n\")]\n",
    "    \n",
    "def pdf_to_text(path: str) -> str:\n",
    "    texts = []\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            t = page.extract_text(x_tolerance=1, y_tolerance=1) or \"\"\n",
    "            if t:\n",
    "                texts.append(t)\n",
    "    raw = \"\\\\n\".join(texts)\n",
    "    raw = raw.replace(\"\\\\r\\\\n\", \"\\\\n\").replace(\"\\\\r\", \"\\\\n\")\n",
    "    lines = raw.split(\"\\\\n\")\n",
    "    lines = clean_lines(lines)\n",
    "    return \"\\\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "964d2f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_MARKERS = [\n",
    "    r\"^Classification$\", r\"^End of Document$\", r\"^Load-Date:\", r\"^Subject:\", r\"^Industry:\",\n",
    "    r\"^Language:\", r\"^Publication-Type:\", r\"^Graphic$\", r\"^Bookmark_\\d+\\s*$\"\n",
    "]\n",
    "STOP_RX = re.compile(\"|\".join(STOP_MARKERS), re.IGNORECASE)\n",
    "\n",
    "def extract_title_lead_body(clean_text: str):\n",
    "    \"\"\"\n",
    "    LexisNexis-structuur:\n",
    "      [koppen/metadata] ... \n",
    "      Title (vaak 1-2x herhaald)\n",
    "      Krant / datum / sectie / lengte / byline ...\n",
    "      Body\n",
    "      <artikeltekst (meerdere alinea's, soms paginabreaks)>\n",
    "      Classification / End of Document / etc.\n",
    "\n",
    "    Output:\n",
    "      title: 1 regel\n",
    "      lead:  korte samenvatting (eerste paragraaf na Body, overslaat 1-woord labels zoals 'Column', 'Geneesmiddelen')\n",
    "      body:  rest tot aan STOP_MARKERS\n",
    "    \"\"\"\n",
    "    # 1) naar regels\n",
    "    lines = [ln.strip() for ln in clean_text.replace(\"\\r\", \"\\n\").split(\"\\n\")]\n",
    "    lines = [ln for ln in lines if ln is not None]  # behoud lege regels als scheiding\n",
    "\n",
    "    # 2) vind titelkandidaat (eerste niet-lege regel die NIET 'Page x of y' / URL / About is)\n",
    "    def is_noise_title(l):\n",
    "        if not l: return True\n",
    "        if re.match(r\"^Page \\d+ of \\d+$\", l): return True\n",
    "        if re.match(r\"^http[s]?://\", l, re.I): return True\n",
    "        if \"About LexisNexis\" in l or \"Privacy Policy\" in l or \"Terms\" in l: return True\n",
    "        return False\n",
    "\n",
    "    first_nonempty = next((i for i,l in enumerate(lines) if l and not is_noise_title(l)), None)\n",
    "    title = lines[first_nonempty] if first_nonempty is not None else \"\"\n",
    "\n",
    "    # 3) vind de EERSTE 'Body' regel (dit markeert begin van inhoud)\n",
    "    body_idx = next((i for i,l in enumerate(lines) if l.strip().lower() == \"body\"), None)\n",
    "    if body_idx is None:\n",
    "        # fallback: soms staat 'Body' met extra tekst eronder; probeer een zachtere match\n",
    "        body_idx = next((i for i,l in enumerate(lines) if re.fullmatch(r\"\\s*Body\\s*\", l, re.I)), None)\n",
    "\n",
    "    # als geen Body gevonden: alles na title als body (zeldzaam)\n",
    "    start_idx = (body_idx + 1) if body_idx is not None else ((first_nonempty + 1) if first_nonempty is not None else 0)\n",
    "\n",
    "    # 4) knip af bij eerste STOP_MARKER\n",
    "    end_idx = None\n",
    "    for j in range(start_idx, len(lines)):\n",
    "        if STOP_RX.match(lines[j] or \"\"):\n",
    "            end_idx = j\n",
    "            break\n",
    "    content_lines = lines[start_idx:end_idx] if end_idx else lines[start_idx:]\n",
    "\n",
    "    # 5) verwijder bekende tussenkopjes / 1-woord labels direct na Body (bv. 'Column', 'Geneesmiddelen')\n",
    "    while content_lines and re.fullmatch(r\"[A-Za-zÀ-ÿ\\-’'`]+\", content_lines[0]):\n",
    "        # laat staan als het duidelijk een zin is (eindigt op .?!)\n",
    "        if re.search(r\"[.!?]$\", content_lines[0]): break\n",
    "        # anders overslaan (één-woord of korte rubriek)\n",
    "        content_lines.pop(0)\n",
    "\n",
    "    # 6) maak paragrafen (lege regel = scheiding; als die ontbreken, per “lege regel” simuleren op dubbele spaties)\n",
    "    raw = \"\\n\".join(content_lines)\n",
    "    # normaliseer meerdere lege regels\n",
    "    raw = re.sub(r\"\\n{3,}\", \"\\n\\n\", raw).strip()\n",
    "    paras = [p.strip() for p in re.split(r\"\\n\\s*\\n\", raw) if p.strip()]\n",
    "    if not paras:\n",
    "        # fallback: forceer paragrafen grofweg op zinsafsluiting\n",
    "        paras = [p.strip() for p in re.split(r\"(?<=[.!?])\\s+\", raw) if p.strip()]\n",
    "\n",
    "    # 7) lead = eerste paragraaf (max 2-3 zinnen), body = rest\n",
    "    if paras:\n",
    "        # knip lead op 2-3 zinnen\n",
    "        sents = re.split(r\"(?<=[.!?])\\s+\", paras[0])\n",
    "        lead = \" \".join(sents[:3]).strip()\n",
    "        remainder = \" \".join(sents[3:]).strip()\n",
    "        body_paras = ([remainder] if remainder else []) + paras[1:]\n",
    "        body = \"\\n\\n\".join(body_paras).strip()\n",
    "    else:\n",
    "        lead, body = \"\", \"\"\n",
    "\n",
    "    return title.strip(), lead, body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24082f78-4a3e-4451-afab-5ba38d91d9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Bericht 007 Nederlandse pati_nt wacht te lang op betere medicijnen tegen kanker.pdf ===\n",
      "TITLE: Nederlandse patiënt wacht te lang op betere medicijnen tegen kanker\n",
      "LEAD : Wat een prachtig bericht onlangs, dat meer kankerpatiënten de afgelopen decennia bleven leven. Twee derde van\n",
      "de patiënten met de diagnose kanker leeft na vijf jaar nog. Een vooruitgang die volgens het Integraal\n",
      "Kankercentrum Nederland mede te danken is aan innovatieve geneesmiddelen tegen gevorderde en uitgezaaide\n",
      "kanker.\n",
      "BODY len: 3697 chars\n",
      "BODY preview: Nog niet voor alle soorten, maar in ieder geval voor huid-, long-, prostaat-, bloed-, nier- en blaaskanker. Dat zijn in aantal niet de minste. Maar het is jammer dat het zo lang duurt voordat dergelijke geneesmiddelen na goedkeuring door de Amerikaanse autoriteiten in de Nederlandse praktijk terecht ...\n",
      "\n",
      "=== Bericht 010_Nieuwe kankermedicijnen leveren meer financi_le winst op dan gezondheidswinst.pdf ===\n",
      "TITLE: Nieuwe kankermedicijnen leveren meer financiële winst op dan gezondheidswinst\n",
      "LEAD : Vorige week verscheen in Trouw een artikel met de prikkelende kop: 'Kankerpatiënt krijgt te laat toegang tot\n",
      "nieuwe medicijnen.' De auteur van dit opiniestuk, zelf sinds 2018 longkankerpatiënt, stelt dat het veel te lang duurt\n",
      "voordat kankermedicijnen beschikbaar komen in ons land. 'Nederland is een mooi land waarin uiteindelijk bijna alle in de VS goedgekeurde kankergeneesmiddelen in het\n",
      "basispakket komen. Maar de wachttijd is te lang.\n",
      "BODY len: 4342 chars\n",
      "BODY preview: Om de kosten hoeven we de toegang tot deze middelen niet uit te stellen', zo beweert hij. Ik vermoed dat Big Pharma zich bij lezing in de handen stond te wrijven. De werkelijkheid is dat er de afgelopen jaren zeer veel innovatieve en dure kankermedicijnen op de markt zijn gekomen. Middelen die in de ...\n",
      "\n",
      "=== Bericht 011_Hoe controleer je verstopte moedervlekken_.pdf ===\n",
      "TITLE: Hoe controleer je verstopte moedervlekken?\n",
      "LEAD : Preventie het consult\n",
      "Meer dan twintig jaar geleden ontdekte ze op haar buik een moedervlek die van kleur veranderde. Het bleek een\n",
      "melanoom, die bovendien was uitgezaaid naar de lymfeklieren. Artsen hebben alle kwaadaardige cellen\n",
      "weggehaald en de tumor is nooit meer teruggekomen.\n",
      "BODY len: 2904 chars\n",
      "BODY preview: Toch zit de schrik er nog goed in en controleert ze elke dag haar lichaam op verdachte vlekjes. Alleen, zo vraagt deze lezer (61) zich af, hoe controleer je moedervlekken onder je haar? Eerst over die dagelijkse inspectie. Dat is echt niet nodig, zegt Soe Janssens, dermatoloog bij het Antoni van Lee ...\n",
      "\n",
      "=== Bericht 016_Ik vind het erg als _n infuus van 25.000 euro wordt weggegooid_.pdf ===\n",
      "TITLE: 'Ik vind het erg als 'n infuus van 25.000 euro wordt weggegooid'\n",
      "LEAD : Slimme ideeën van apotheker Roelof van Leeuwen besparen miljoenen\n",
      "Ellen van Gaalen\n",
      "Rotterdam\n",
      "Waarom schrijven artsen 1005 milligram van een oncologisch middel voor, terwijl de verpakkingen per 100\n",
      "milligram gaan? ,,Dan gooien we dus 95 procent weg. Dat zijn vaak dure medicijnen.\" Eenmaal aan een patiënt\n",
      "gegeven middelen mogen na inlevering niet aan een andere patiënt worden gegeven.\n",
      "BODY len: 4858 chars\n",
      "BODY preview: Ziekenhuisapotheker Roelof van Leeuwen zet zich al jaren in voor doelmatiger gebruik van oncologische middelen. ,,Ik vind het erg als een infuus ter waarde van 25.000 euro wordt weggegooid\", zegt hij. Daarom is het zijn missie om zuiniger met medicijnen tegen kanker te leren omgaan. Sinds 2016 werkt ...\n"
     ]
    }
   ],
   "source": [
    "test_files = [\n",
    "    \"data/Bericht 007 Nederlandse pati_nt wacht te lang op betere medicijnen tegen kanker.pdf\",\n",
    "    \"data/Bericht 010_Nieuwe kankermedicijnen leveren meer financi_le winst op dan gezondheidswinst.pdf\",\n",
    "    \"data/Bericht 011_Hoe controleer je verstopte moedervlekken_.pdf\",\n",
    "    \"data/Bericht 016_Ik vind het erg als _n infuus van 25.000 euro wordt weggegooid_.pdf\",\n",
    "]\n",
    "\n",
    "for fp in test_files:\n",
    "    txt = pdf_to_text(fp)           # jouw bestaande pdf->text + clean\n",
    "    title, lead, body = extract_title_lead_body(txt)\n",
    "    print(\"\\n===\", os.path.basename(fp), \"===\")\n",
    "    print(\"TITLE:\", title[:120])\n",
    "    print(\"LEAD :\", lead)\n",
    "    print(\"BODY len:\", len(body), \"chars\")\n",
    "    print(\"BODY preview:\", body[:300].replace(\"\\n\",\" \") + \" ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40c15df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chunks_by_tokens(text: str, tokenizer, max_tokens: int = 400, stride: int = 50) -> List[str]:\n",
    "    if not text.strip():\n",
    "        return []\n",
    "    toks = tokenizer.encode(text, add_special_tokens=False)\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(toks):\n",
    "        window = toks[i:i+max_tokens]\n",
    "        if not window:\n",
    "            break\n",
    "        chunk = tokenizer.decode(window, skip_special_tokens=True)\n",
    "        chunks.append(chunk)\n",
    "        if i + max_tokens >= len(toks):\n",
    "            break\n",
    "        i += max_tokens - stride\n",
    "    return chunks\n",
    "\n",
    "def chunk_body(text: str, tokenizer, prefer_paragraphs: bool = True, max_tokens: int = 400, stride: int = 50) -> List[str]:\n",
    "    if not text.strip():\n",
    "        return []\n",
    "    if prefer_paragraphs:\n",
    "        paras = re.split(r\"\\\\n\\\\s*\\\\n\", text.strip())\n",
    "        paras = [p.strip() for p in paras if p.strip()]\n",
    "        chunks = []\n",
    "        for p in paras:\n",
    "            if len(tokenizer.encode(p, add_special_tokens=False)) <= max_tokens:\n",
    "                chunks.append(p)\n",
    "            else:\n",
    "                chunks.extend(make_chunks_by_tokens(p, tokenizer, max_tokens=max_tokens, stride=stride))\n",
    "        return chunks\n",
    "    else:\n",
    "        return make_chunks_by_tokens(text, tokenizer, max_tokens=max_tokens, stride=stride)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da6aa4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Loaded: DTAI-KULeuven/robbert-v2-dutch-sentiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Loaded: DTAI-KULeuven/robbertje-merged-dutch-sentiment\n",
      "[OK] Chunking tokenizer ready\n"
     ]
    }
   ],
   "source": [
    "ROB_NAME = \"DTAI-KULeuven/robbert-v2-dutch-sentiment\"\n",
    "BER_NAME = \"DTAI-KULeuven/robbertje-merged-dutch-sentiment\"\n",
    "\n",
    "def load_pipe(name, tries=2):\n",
    "    last = None\n",
    "    for i in range(tries):\n",
    "        try:\n",
    "            clf = pipeline(\n",
    "                task=\"sentiment-analysis\",\n",
    "                model=name,\n",
    "                tokenizer=name,\n",
    "                top_k=None,          # vervangt return_all_scores=True\n",
    "                truncation=True\n",
    "            )\n",
    "            print(f\"[OK] Loaded: {name}\")\n",
    "            return clf\n",
    "        except Exception as e:\n",
    "            last = e\n",
    "            print(f\"[WARN] {name} attempt {i+1} failed: {e}\")\n",
    "            time.sleep(3)\n",
    "    raise RuntimeError(f\"Kon {name} niet laden: {last}\")\n",
    "\n",
    "rob_pipe = load_pipe(ROB_NAME)\n",
    "ber_pipe = load_pipe(BER_NAME)\n",
    "\n",
    "# Tokenizer voor chunking (pak RobBERT als die live is, anders BERTje)\n",
    "try:\n",
    "    TOKENIZER = AutoTokenizer.from_pretrained(ROB_NAME)\n",
    "except Exception:\n",
    "    TOKENIZER = AutoTokenizer.from_pretrained(BER_NAME)\n",
    "print(\"[OK] Chunking tokenizer ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a58e760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MODEL_TOKENS = 512       # limiet model\n",
    "HEADROOM = 8                  # veiligheidsmarge\n",
    "SAFE_MAX = MAX_MODEL_TOKENS - HEADROOM  # 504 tokens\n",
    "\n",
    "def token_chunks(text, tokenizer, max_tokens=SAFE_MAX, stride=50):\n",
    "    \"\"\"\n",
    "    Splits lange tekst in chunks zodat ze <512 tokens blijven.\n",
    "    \"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return []\n",
    "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    if len(ids) <= max_tokens:\n",
    "        return [text]\n",
    "\n",
    "    out = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        window = ids[i:i+max_tokens]\n",
    "        if not window: \n",
    "            break\n",
    "        out.append(tokenizer.decode(window, skip_special_tokens=True))\n",
    "        if i + max_tokens >= len(ids): \n",
    "            break\n",
    "        i += max_tokens - stride\n",
    "    return out\n",
    "\n",
    "def split_title_lead_if_needed(title, lead, tokenizer):\n",
    "    title_chs = token_chunks(title, tokenizer) if title else []\n",
    "    lead_chs  = token_chunks(lead, tokenizer) if lead  else []\n",
    "    return title_chs, lead_chs\n",
    "\n",
    "\n",
    "LABEL_MAP = {\n",
    "    \"POSITIVE\": \"positief\", \"NEGATIVE\": \"negatief\", \"NEUTRAL\": \"neutraal\",\n",
    "    \"Positive\": \"positief\", \"Negative\": \"negatief\", \"Neutral\": \"neutraal\",\n",
    "    \"positief\": \"positief\", \"negatief\": \"negatief\", \"neutraal\": \"neutraal\"\n",
    "}\n",
    "\n",
    "def normalize_pnn(probs):\n",
    "    arr = np.array([probs.get(\"positief\",0), probs.get(\"negatief\",0), probs.get(\"neutraal\",0)], dtype=float)\n",
    "    s = arr.sum()\n",
    "    if s <= 0: return {\"positief\":0.0, \"negatief\":0.0, \"neutraal\":1.0}\n",
    "    arr = arr/s\n",
    "    return {\"positief\": float(arr[0]), \"negatief\": float(arr[1]), \"neutraal\": float(arr[2])}\n",
    "\n",
    "def score_text_with_pipe(text: str, clf) -> dict:\n",
    "    if not text.strip():\n",
    "        return {\"positief\":0.0, \"negatief\":0.0, \"neutraal\":1.0}\n",
    "    out = clf(text, truncation=True)   # bij top_k=None: lijst van dicts\n",
    "    scores = out[0]\n",
    "    probs = {\"positief\":0.0, \"negatief\":0.0, \"neutraal\":0.0}\n",
    "    for item in scores:\n",
    "        lab = LABEL_MAP.get(item[\"label\"])\n",
    "        if lab: probs[lab] = float(item[\"score\"])\n",
    "    return normalize_pnn(probs)\n",
    "\n",
    "def aggregate_article_with_pipe(title, lead, body_chunks, clf, tokenizer,\n",
    "                                weights=WEIGHTS, neutral_band=NEUTRAL_BAND):\n",
    "    \"\"\"\n",
    "    - Chunk ook titel en lead naar <512 tokens.\n",
    "    - Verdeel gewicht gelijkmatig over alle chunks.\n",
    "    - Zorgt dat het model veilig kan draaien zonder warnings/errors.\n",
    "    \"\"\"\n",
    "    # Chunk titel en lead\n",
    "    title_chunks, lead_chunks = split_title_lead_if_needed(title, lead, tokenizer)\n",
    "\n",
    "    parts = []\n",
    "    if title_chunks:\n",
    "        w_each = weights.title / len(title_chunks)\n",
    "        for t in title_chunks:\n",
    "            parts.append((t, w_each))\n",
    "    if lead_chunks:\n",
    "        w_each = weights.lead / len(lead_chunks)\n",
    "        for l in lead_chunks:\n",
    "            parts.append((l, w_each))\n",
    "    for ch in body_chunks:\n",
    "        if ch.strip():\n",
    "            parts.append((ch.strip(), weights.body))\n",
    "\n",
    "    if not parts:\n",
    "        return {\"p_pos\":0.0, \"p_neg\":0.0, \"p_neu\":1.0, \"score\":0.0, \"label\":\"neutraal\"}\n",
    "\n",
    "    acc = np.zeros(3, dtype=float)\n",
    "    diffs, wts = [], []\n",
    "    for txt, w in parts:\n",
    "        # model call, forceren max_length\n",
    "        out = clf(txt, truncation=True, max_length=MAX_MODEL_TOKENS, padding=False)\n",
    "        scores = out[0]  # top_k=None -> lijst van dicts\n",
    "        probs = {\"positief\":0.0,\"negatief\":0.0,\"neutraal\":0.0}\n",
    "        for item in scores:\n",
    "            lab = LABEL_MAP.get(item[\"label\"])\n",
    "            if lab: probs[lab] = float(item[\"score\"])\n",
    "        probs = normalize_pnn(probs)\n",
    "\n",
    "        acc   += np.array([probs[\"positief\"], probs[\"negatief\"], probs[\"neutraal\"]]) * w\n",
    "        diffs += [(probs[\"positief\"] - probs[\"negatief\"]) * w]\n",
    "        wts   += [w]\n",
    "\n",
    "    W = max(sum(wts), 1e-9)\n",
    "    p_pos, p_neg, p_neu = (acc / W).tolist()\n",
    "    signed = float(sum(diffs) / W)\n",
    "    label = \"neutraal\" if abs(signed) < neutral_band else (\"positief\" if signed > 0 else \"negatief\")\n",
    "    return {\"p_pos\":p_pos, \"p_neg\":p_neg, \"p_neu\":p_neu, \"score\":signed, \"label\":label}\n",
    "\n",
    "def avg_two(prob_a: dict, prob_b: dict) -> dict:\n",
    "    # neem gemiddelde van p_pos/p_neg/p_neu uit twee modellen\n",
    "    p = {\n",
    "        \"p_pos\": (prob_a[\"p_pos\"] + prob_b[\"p_pos\"]) / 2.0,\n",
    "        \"p_neg\": (prob_a[\"p_neg\"] + prob_b[\"p_neg\"]) / 2.0,\n",
    "        \"p_neu\": (prob_a[\"p_neu\"] + prob_b[\"p_neu\"]) / 2.0,\n",
    "    }\n",
    "    signed = (p[\"p_pos\"] - p[\"p_neg\"])\n",
    "    p[\"score\"] = signed\n",
    "    p[\"label\"] = \"neutraal\" if abs(signed) < NEUTRAL_BAND else (\"positief\" if signed > 0 else \"negatief\")\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44dbc295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PDFs verwerken:   0%|                                                                           | 0/70 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (750 > 512). Running this sequence through the model will result in indexing errors\n",
      "PDFs verwerken: 100%|██████████████████████████████████████████████████████████████████| 70/70 [03:28<00:00,  2.97s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>lead</th>\n",
       "      <th>n_body_chunks</th>\n",
       "      <th>rob_p_pos</th>\n",
       "      <th>rob_p_neg</th>\n",
       "      <th>rob_p_neu</th>\n",
       "      <th>rob_score</th>\n",
       "      <th>rob_label</th>\n",
       "      <th>ber_p_pos</th>\n",
       "      <th>ber_p_neg</th>\n",
       "      <th>ber_p_neu</th>\n",
       "      <th>ber_score</th>\n",
       "      <th>ber_label</th>\n",
       "      <th>ens_p_pos</th>\n",
       "      <th>ens_p_neg</th>\n",
       "      <th>ens_p_neu</th>\n",
       "      <th>ens_score</th>\n",
       "      <th>ens_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bericht 007 Nederlandse pati_nt wacht te lang ...</td>\n",
       "      <td>Nederlandse patiënt wacht te lang op betere me...</td>\n",
       "      <td>Wat een prachtig bericht onlangs, dat meer kan...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.302636</td>\n",
       "      <td>0.697364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.394728</td>\n",
       "      <td>negatief</td>\n",
       "      <td>0.285786</td>\n",
       "      <td>0.714214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.428428</td>\n",
       "      <td>negatief</td>\n",
       "      <td>0.294211</td>\n",
       "      <td>0.705789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.411578</td>\n",
       "      <td>negatief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bericht 010_Nieuwe kankermedicijnen leveren me...</td>\n",
       "      <td>Nieuwe kankermedicijnen leveren meer financiël...</td>\n",
       "      <td>Vorige week verscheen in Trouw een artikel met...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.432665</td>\n",
       "      <td>0.567335</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.134670</td>\n",
       "      <td>negatief</td>\n",
       "      <td>0.005389</td>\n",
       "      <td>0.994611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.989222</td>\n",
       "      <td>negatief</td>\n",
       "      <td>0.219027</td>\n",
       "      <td>0.780973</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.561946</td>\n",
       "      <td>negatief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bericht 011_Hoe controleer je verstopte moeder...</td>\n",
       "      <td>Hoe controleer je verstopte moedervlekken?</td>\n",
       "      <td>Preventie het consult\\nMeer dan twintig jaar g...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.882563</td>\n",
       "      <td>0.117437</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.765126</td>\n",
       "      <td>positief</td>\n",
       "      <td>0.664079</td>\n",
       "      <td>0.335921</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.328157</td>\n",
       "      <td>positief</td>\n",
       "      <td>0.773321</td>\n",
       "      <td>0.226679</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.546642</td>\n",
       "      <td>positief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bericht 016_Ik vind het erg als _n infuus van ...</td>\n",
       "      <td>'Ik vind het erg als 'n infuus van 25.000 euro...</td>\n",
       "      <td>Slimme ideeën van apotheker Roelof van Leeuwen...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.502075</td>\n",
       "      <td>0.497925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004151</td>\n",
       "      <td>neutraal</td>\n",
       "      <td>0.218087</td>\n",
       "      <td>0.781913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.563827</td>\n",
       "      <td>negatief</td>\n",
       "      <td>0.360081</td>\n",
       "      <td>0.639919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.279838</td>\n",
       "      <td>negatief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bericht 021_Wachtlijsten en personeelstekort_ ...</td>\n",
       "      <td>Wachtlijsten en personeelstekort: het 'zorginf...</td>\n",
       "      <td>Onbetaalbare zorg\\nDe gezondheidszorg kan het ...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.611262</td>\n",
       "      <td>0.388738</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.222525</td>\n",
       "      <td>positief</td>\n",
       "      <td>0.313417</td>\n",
       "      <td>0.686583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.373166</td>\n",
       "      <td>negatief</td>\n",
       "      <td>0.462340</td>\n",
       "      <td>0.537660</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.075321</td>\n",
       "      <td>neutraal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id  \\\n",
       "0  Bericht 007 Nederlandse pati_nt wacht te lang ...   \n",
       "1  Bericht 010_Nieuwe kankermedicijnen leveren me...   \n",
       "2  Bericht 011_Hoe controleer je verstopte moeder...   \n",
       "3  Bericht 016_Ik vind het erg als _n infuus van ...   \n",
       "4  Bericht 021_Wachtlijsten en personeelstekort_ ...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Nederlandse patiënt wacht te lang op betere me...   \n",
       "1  Nieuwe kankermedicijnen leveren meer financiël...   \n",
       "2         Hoe controleer je verstopte moedervlekken?   \n",
       "3  'Ik vind het erg als 'n infuus van 25.000 euro...   \n",
       "4  Wachtlijsten en personeelstekort: het 'zorginf...   \n",
       "\n",
       "                                                lead  n_body_chunks  \\\n",
       "0  Wat een prachtig bericht onlangs, dat meer kan...              2   \n",
       "1  Vorige week verscheen in Trouw een artikel met...              3   \n",
       "2  Preventie het consult\\nMeer dan twintig jaar g...              2   \n",
       "3  Slimme ideeën van apotheker Roelof van Leeuwen...              3   \n",
       "4  Onbetaalbare zorg\\nDe gezondheidszorg kan het ...              8   \n",
       "\n",
       "   rob_p_pos  rob_p_neg  rob_p_neu  rob_score rob_label  ber_p_pos  ber_p_neg  \\\n",
       "0   0.302636   0.697364        0.0  -0.394728  negatief   0.285786   0.714214   \n",
       "1   0.432665   0.567335        0.0  -0.134670  negatief   0.005389   0.994611   \n",
       "2   0.882563   0.117437        0.0   0.765126  positief   0.664079   0.335921   \n",
       "3   0.502075   0.497925        0.0   0.004151  neutraal   0.218087   0.781913   \n",
       "4   0.611262   0.388738        0.0   0.222525  positief   0.313417   0.686583   \n",
       "\n",
       "   ber_p_neu  ber_score ber_label  ens_p_pos  ens_p_neg  ens_p_neu  ens_score  \\\n",
       "0        0.0  -0.428428  negatief   0.294211   0.705789        0.0  -0.411578   \n",
       "1        0.0  -0.989222  negatief   0.219027   0.780973        0.0  -0.561946   \n",
       "2        0.0   0.328157  positief   0.773321   0.226679        0.0   0.546642   \n",
       "3        0.0  -0.563827  negatief   0.360081   0.639919        0.0  -0.279838   \n",
       "4        0.0  -0.373166  negatief   0.462340   0.537660        0.0  -0.075321   \n",
       "\n",
       "  ens_label  \n",
       "0  negatief  \n",
       "1  negatief  \n",
       "2  positief  \n",
       "3  negatief  \n",
       "4  neutraal  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rows = []\n",
    "pdf_files = [f for f in sorted(os.listdir(DATA_DIR)) if f.lower().endswith('.pdf')]\n",
    "if not pdf_files:\n",
    "    print(f\"[INFO] Geen PDF-bestanden gevonden in '{DATA_DIR}'.\")\n",
    "else:\n",
    "    for fname in tqdm(pdf_files, desc=\"PDFs verwerken\"):\n",
    "        fpath = os.path.join(DATA_DIR, fname)\n",
    "        clean_txt = pdf_to_text(fpath)\n",
    "        title, lead, body = extract_title_lead_body(clean_txt)\n",
    "        body_chunks = chunk_body(body, TOKENIZER, prefer_paragraphs=True, max_tokens=MAX_TOKENS, stride=STRIDE)\n",
    "\n",
    "        # per model\n",
    "        rob = aggregate_article_with_pipe(title, lead, body_chunks, rob_pipe, TOKENIZER)\n",
    "        ber = aggregate_article_with_pipe(title, lead, body_chunks, ber_pipe, TOKENIZER)\n",
    "        ens = avg_two({\"p_pos\":rob[\"p_pos\"], \"p_neg\":rob[\"p_neg\"], \"p_neu\":rob[\"p_neu\"]},\n",
    "                      {\"p_pos\":ber[\"p_pos\"], \"p_neg\":ber[\"p_neg\"], \"p_neu\":ber[\"p_neu\"]})\n",
    "\n",
    "        rows.append({\n",
    "            \"id\": os.path.splitext(fname)[0],\n",
    "            \"title\": title,\n",
    "            \"lead\": lead,\n",
    "            \"n_body_chunks\": len(body_chunks),\n",
    "\n",
    "            # RobBERT\n",
    "            \"rob_p_pos\": rob[\"p_pos\"], \"rob_p_neg\": rob[\"p_neg\"], \"rob_p_neu\": rob[\"p_neu\"],\n",
    "            \"rob_score\": rob[\"score\"], \"rob_label\": rob[\"label\"],\n",
    "\n",
    "            # BERTje\n",
    "            \"ber_p_pos\": ber[\"p_pos\"], \"ber_p_neg\": ber[\"p_neg\"], \"ber_p_neu\": ber[\"p_neu\"],\n",
    "            \"ber_score\": ber[\"score\"], \"ber_label\": ber[\"label\"],\n",
    "\n",
    "            # Ensemble (gemiddelde van probs)\n",
    "            \"ens_p_pos\": ens[\"p_pos\"], \"ens_p_neg\": ens[\"p_neg\"], \"ens_p_neu\": ens[\"p_neu\"],\n",
    "            \"ens_score\": ens[\"score\"], \"ens_label\": ens[\"label\"],\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "54bfa0aa-81b3-41b1-9644-ff6947c5bbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Geschreven naar sentiment_results.csv\n"
     ]
    }
   ],
   "source": [
    "    df.to_csv(\"sentiment_results.csv\", index=False, encoding=\"utf-8\")\n",
    "    print(\"[DONE] Geschreven naar sentiment_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9e99f9-943f-4699-88b8-f8ec5da36dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
