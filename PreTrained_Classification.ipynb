{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c124118c-ed63-4e32-b29a-2f8f8394b35d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26f50599-fcb5-4727-aac0-f5fd9d92ed5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\semde\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] Kan opgegeven procedure niet vinden'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os, re, math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pdfplumber\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd6375c-d3ad-45d2-914a-d825030873b5",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e94fd5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Config: RobBERT v2 + neutral band en gewichten (titel/lead/body) ===\n",
    "ROB_BERT_SENTIMENT = \"DTAI-KULeuven/robbert-v2-dutch-sentiment\"\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "OUT_CSV  = \"sentiment_results.csv\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Weights:\n",
    "    title: float = 1.0\n",
    "    lead:  float = 1.0\n",
    "    body:  float = 1.0   # body weer meewegen\n",
    "\n",
    "WEIGHTS = Weights()\n",
    "\n",
    "# Neutral band voor beslissing op basis van signed score (p_pos - p_neg)\n",
    "NEUTRAL_BAND = 0.1\n",
    "\n",
    "# Chunk-instellingen (gebruik je al elders)\n",
    "MAX_TOKENS = 400\n",
    "STRIDE     = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6099e5-28bd-400d-9d31-bd75fc876724",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f202cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEXIS_HEADER_PATTERNS = [\n",
    "    r\"^About LexisNexis.*\",\n",
    "    r\"^Privacy Policy.*\",\n",
    "    r\"^Terms .* Conditions.*\",\n",
    "    r\"^Copyright.*\",\n",
    "    r\"^User Name:.*\",\n",
    "    r\"^Date and Time:.*\",\n",
    "    r\"^Job Number:.*\",\n",
    "    r\"^Documents \\\\(\\\\d+\\\\).*\",\n",
    "    r\"^Client/Matter:.*\",\n",
    "    r\"^Search Terms:.*\",\n",
    "    r\"^Search Type:.*\",\n",
    "    r\"^Content Type.*\",\n",
    "    r\"^http[s]?://\\\\S+\",\n",
    "    r\"^Page \\\\d+ of \\\\d+\",\n",
    "    r\"^Load-Date:.*\",\n",
    "    r\"^End of Document\",\n",
    "    r\"^Classification$\",\n",
    "    r\"^Language:.*\",\n",
    "    r\"^Publication-Type:.*\",\n",
    "    r\"^Subject:.*\",\n",
    "    r\"^Industry:.*\",\n",
    "    r\"^\\\\s*Bookmark_\\\\d+\\\\s*$\"\n",
    "]\n",
    "\n",
    "HEADER_REGEXES = [re.compile(pat, flags=re.IGNORECASE) for pat in LEXIS_HEADER_PATTERNS]\n",
    "\n",
    "def clean_lines(lines: List[str]) -> List[str]:\n",
    "    out = []\n",
    "    for ln in lines:\n",
    "        s = ln.strip()\n",
    "        if not s:\n",
    "            out.append(\"\")\n",
    "            continue\n",
    "        if any(rx.match(s) for rx in HEADER_REGEXES):\n",
    "            continue\n",
    "        s = re.sub(r\"\\\\s+\", \" \", s)\n",
    "        out.append(s)\n",
    "    txt = \"\\\\n\".join(out)\n",
    "    txt = re.sub(r\"\\\\n{3,}\", \"\\\\n\\\\n\", txt)\n",
    "    return [ln for ln in txt.split(\"\\\\n\")]\n",
    "    \n",
    "def pdf_to_text(path: str) -> str:\n",
    "    texts = []\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            t = page.extract_text(x_tolerance=1, y_tolerance=1) or \"\"\n",
    "            if t:\n",
    "                texts.append(t)\n",
    "    raw = \"\\\\n\".join(texts)\n",
    "    raw = raw.replace(\"\\\\r\\\\n\", \"\\\\n\").replace(\"\\\\r\", \"\\\\n\")\n",
    "    lines = raw.split(\"\\\\n\")\n",
    "    lines = clean_lines(lines)\n",
    "    return \"\\\\n\".join(lines)\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    if not s:\n",
    "        return s\n",
    "    # snelle mojibake-detectie en herstel (bv. 'patiÃ«nt' → 'patiënt')\n",
    "    if re.search(r\"[ÂÃ]|Ã.|â€|â€™|â€œ|â€�|â€“|â€”\", s):\n",
    "        try:\n",
    "            s = s.encode(\"latin-1\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    # unicode normalisatie + wat typografische tekens rechtzetten\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    s = (s.replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\")\n",
    "           .replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "           .replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
    "           .replace(\"\\u00ad\", \"\"))  # soft hyphen\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "964d2f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_MARKERS = [\n",
    "    r\"^Classification$\", r\"^End of Document$\", r\"^Load-Date:\", r\"^Subject:\", r\"^Industry:\",\n",
    "    r\"^Language:\", r\"^Publication-Type:\", r\"^Graphic$\", r\"^Bookmark_\\d+\\s*$\"\n",
    "]\n",
    "STOP_RX = re.compile(\"|\".join(STOP_MARKERS), re.IGNORECASE)\n",
    "\n",
    "def extract_title_lead_body(clean_text: str):\n",
    "    \"\"\"\n",
    "    LexisNexis-structuur:\n",
    "      [koppen/metadata] ... \n",
    "      Title (vaak 1-2x herhaald)\n",
    "      Krant / datum / sectie / lengte / byline ...\n",
    "      Body\n",
    "      <artikeltekst (meerdere alinea's, soms paginabreaks)>\n",
    "      Classification / End of Document / etc.\n",
    "\n",
    "    Output:\n",
    "      title: 1 regel\n",
    "      lead:  korte samenvatting (eerste paragraaf na Body, overslaat 1-woord labels zoals 'Column', 'Geneesmiddelen')\n",
    "      body:  rest tot aan STOP_MARKERS\n",
    "    \"\"\"\n",
    "    # 1) naar regels\n",
    "    lines = [ln.strip() for ln in clean_text.replace(\"\\r\", \"\\n\").split(\"\\n\")]\n",
    "    lines = [ln for ln in lines if ln is not None]  # behoud lege regels als scheiding\n",
    "\n",
    "    # 2) vind titelkandidaat (eerste niet-lege regel die NIET 'Page x of y' / URL / About is)\n",
    "    def is_noise_title(l):\n",
    "        if not l: return True\n",
    "        if re.match(r\"^Page \\d+ of \\d+$\", l): return True\n",
    "        if re.match(r\"^http[s]?://\", l, re.I): return True\n",
    "        if \"About LexisNexis\" in l or \"Privacy Policy\" in l or \"Terms\" in l: return True\n",
    "        return False\n",
    "\n",
    "    first_nonempty = next((i for i,l in enumerate(lines) if l and not is_noise_title(l)), None)\n",
    "    title = lines[first_nonempty] if first_nonempty is not None else \"\"\n",
    "\n",
    "    # 3) vind de EERSTE 'Body' regel (dit markeert begin van inhoud)\n",
    "    body_idx = next((i for i,l in enumerate(lines) if l.strip().lower() == \"body\"), None)\n",
    "    if body_idx is None:\n",
    "        # fallback: soms staat 'Body' met extra tekst eronder; probeer een zachtere match\n",
    "        body_idx = next((i for i,l in enumerate(lines) if re.fullmatch(r\"\\s*Body\\s*\", l, re.I)), None)\n",
    "\n",
    "    # als geen Body gevonden: alles na title als body (zeldzaam)\n",
    "    start_idx = (body_idx + 1) if body_idx is not None else ((first_nonempty + 1) if first_nonempty is not None else 0)\n",
    "\n",
    "    # 4) knip af bij eerste STOP_MARKER\n",
    "    end_idx = None\n",
    "    for j in range(start_idx, len(lines)):\n",
    "        if STOP_RX.match(lines[j] or \"\"):\n",
    "            end_idx = j\n",
    "            break\n",
    "    content_lines = lines[start_idx:end_idx] if end_idx else lines[start_idx:]\n",
    "\n",
    "    # 5) verwijder bekende tussenkopjes / 1-woord labels direct na Body (bv. 'Column', 'Geneesmiddelen')\n",
    "    while content_lines and re.fullmatch(r\"[A-Za-zÀ-ÿ\\-’'`]+\", content_lines[0]):\n",
    "        # laat staan als het duidelijk een zin is (eindigt op .?!)\n",
    "        if re.search(r\"[.!?]$\", content_lines[0]): break\n",
    "        # anders overslaan (één-woord of korte rubriek)\n",
    "        content_lines.pop(0)\n",
    "\n",
    "    # 6) maak paragrafen (lege regel = scheiding; als die ontbreken, per “lege regel” simuleren op dubbele spaties)\n",
    "    raw = \"\\n\".join(content_lines)\n",
    "    # normaliseer meerdere lege regels\n",
    "    raw = re.sub(r\"\\n{3,}\", \"\\n\\n\", raw).strip()\n",
    "    paras = [p.strip() for p in re.split(r\"\\n\\s*\\n\", raw) if p.strip()]\n",
    "    if not paras:\n",
    "        # fallback: forceer paragrafen grofweg op zinsafsluiting\n",
    "        paras = [p.strip() for p in re.split(r\"(?<=[.!?])\\s+\", raw) if p.strip()]\n",
    "\n",
    "    # 7) lead = eerste paragraaf (max 2-3 zinnen), body = rest\n",
    "    if paras:\n",
    "        # knip lead op 2-3 zinnen\n",
    "        sents = re.split(r\"(?<=[.!?])\\s+\", paras[0])\n",
    "        lead = \" \".join(sents[:3]).strip()\n",
    "        remainder = \" \".join(sents[3:]).strip()\n",
    "        body_paras = ([remainder] if remainder else []) + paras[1:]\n",
    "        body = \"\\n\\n\".join(body_paras).strip()\n",
    "    else:\n",
    "        lead, body = \"\", \"\"\n",
    "\n",
    "    title = normalize_text(title)\n",
    "    lead  = normalize_text(lead)\n",
    "    body  = normalize_text(body)\n",
    "\n",
    "    return title.strip(), lead, body"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a406f548-c35c-4c3c-b279-02ac40ca147a",
   "metadata": {},
   "source": [
    "# Test PDF extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24082f78-4a3e-4451-afab-5ba38d91d9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Bericht 007 Nederlandse pati_nt wacht te lang op betere medicijnen tegen kanker.pdf ===\n",
      "TITLE: Nederlandse patiënt wacht te lang op betere medicijnen tegen kanker\n",
      "LEAD : Wat een prachtig bericht onlangs, dat meer kankerpatiënten de afgelopen decennia bleven leven. Twee derde van de patiënten met de diagnose kanker leeft na vijf jaar nog. Een vooruitgang die volgens het Integraal Kankercentrum Nederland mede te danken is aan innovatieve geneesmiddelen tegen gevorderde en uitgezaaide kanker.\n",
      "BODY len: 3697 chars\n",
      "BODY preview: Nog niet voor alle soorten, maar in ieder geval voor huid-, long-, prostaat-, bloed-, nier- en blaaskanker. Dat zijn in aantal niet de minste. Maar het is jammer dat het zo lang duurt voordat dergelijke geneesmiddelen na goedkeuring door de Amerikaanse autoriteiten in de Nederlandse praktijk terecht ...\n",
      "\n",
      "=== Bericht 010_Nieuwe kankermedicijnen leveren meer financi_le winst op dan gezondheidswinst.pdf ===\n",
      "TITLE: Nieuwe kankermedicijnen leveren meer financiële winst op dan gezondheidswinst\n",
      "LEAD : Vorige week verscheen in Trouw een artikel met de prikkelende kop: 'Kankerpatiënt krijgt te laat toegang tot nieuwe medicijnen.' De auteur van dit opiniestuk, zelf sinds 2018 longkankerpatiënt, stelt dat het veel te lang duurt voordat kankermedicijnen beschikbaar komen in ons land. 'Nederland is een mooi land waarin uiteindelijk bijna alle in de VS goedgekeurde kankergeneesmiddelen in het basispakket komen. Maar de wachttijd is te lang.\n",
      "BODY len: 4342 chars\n",
      "BODY preview: Om de kosten hoeven we de toegang tot deze middelen niet uit te stellen', zo beweert hij. Ik vermoed dat Big Pharma zich bij lezing in de handen stond te wrijven. De werkelijkheid is dat er de afgelopen jaren zeer veel innovatieve en dure kankermedicijnen op de markt zijn gekomen. Middelen die in de ...\n",
      "\n",
      "=== Bericht 011_Hoe controleer je verstopte moedervlekken_.pdf ===\n",
      "TITLE: Hoe controleer je verstopte moedervlekken?\n",
      "LEAD : Preventie het consult Meer dan twintig jaar geleden ontdekte ze op haar buik een moedervlek die van kleur veranderde. Het bleek een melanoom, die bovendien was uitgezaaid naar de lymfeklieren. Artsen hebben alle kwaadaardige cellen weggehaald en de tumor is nooit meer teruggekomen.\n",
      "BODY len: 2904 chars\n",
      "BODY preview: Toch zit de schrik er nog goed in en controleert ze elke dag haar lichaam op verdachte vlekjes. Alleen, zo vraagt deze lezer (61) zich af, hoe controleer je moedervlekken onder je haar? Eerst over die dagelijkse inspectie. Dat is echt niet nodig, zegt Soe Janssens, dermatoloog bij het Antoni van Lee ...\n",
      "\n",
      "=== Bericht 016_Ik vind het erg als _n infuus van 25.000 euro wordt weggegooid_.pdf ===\n",
      "TITLE: 'Ik vind het erg als 'n infuus van 25.000 euro wordt weggegooid'\n",
      "LEAD : Slimme ideeën van apotheker Roelof van Leeuwen besparen miljoenen Ellen van Gaalen Rotterdam Waarom schrijven artsen 1005 milligram van een oncologisch middel voor, terwijl de verpakkingen per 100 milligram gaan? ,,Dan gooien we dus 95 procent weg. Dat zijn vaak dure medicijnen.\" Eenmaal aan een patiënt gegeven middelen mogen na inlevering niet aan een andere patiënt worden gegeven.\n",
      "BODY len: 4858 chars\n",
      "BODY preview: Ziekenhuisapotheker Roelof van Leeuwen zet zich al jaren in voor doelmatiger gebruik van oncologische middelen. ,,Ik vind het erg als een infuus ter waarde van 25.000 euro wordt weggegooid\", zegt hij. Daarom is het zijn missie om zuiniger met medicijnen tegen kanker te leren omgaan. Sinds 2016 werkt ...\n"
     ]
    }
   ],
   "source": [
    "test_files = [\n",
    "    \"data/Bericht 007 Nederlandse pati_nt wacht te lang op betere medicijnen tegen kanker.pdf\",\n",
    "    \"data/Bericht 010_Nieuwe kankermedicijnen leveren meer financi_le winst op dan gezondheidswinst.pdf\",\n",
    "    \"data/Bericht 011_Hoe controleer je verstopte moedervlekken_.pdf\",\n",
    "    \"data/Bericht 016_Ik vind het erg als _n infuus van 25.000 euro wordt weggegooid_.pdf\",\n",
    "]\n",
    "\n",
    "for fp in test_files:\n",
    "    txt = pdf_to_text(fp)           # jouw bestaande pdf->text + clean\n",
    "    title, lead, body = extract_title_lead_body(txt)\n",
    "    print(\"\\n===\", os.path.basename(fp), \"===\")\n",
    "    print(\"TITLE:\", title[:120])\n",
    "    print(\"LEAD :\", lead)\n",
    "    print(\"BODY len:\", len(body), \"chars\")\n",
    "    print(\"BODY preview:\", body[:300].replace(\"\\n\",\" \") + \" ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40c15df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chunks_by_tokens(text: str, tokenizer, max_tokens: int = 400, stride: int = 50) -> List[str]:\n",
    "    if not text.strip():\n",
    "        return []\n",
    "    toks = tokenizer.encode(text, add_special_tokens=False)\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(toks):\n",
    "        window = toks[i:i+max_tokens]\n",
    "        if not window:\n",
    "            break\n",
    "        chunk = tokenizer.decode(window, skip_special_tokens=True)\n",
    "        chunks.append(chunk)\n",
    "        if i + max_tokens >= len(toks):\n",
    "            break\n",
    "        i += max_tokens - stride\n",
    "    return chunks\n",
    "\n",
    "def chunk_body(text: str, tokenizer, prefer_paragraphs: bool = True, max_tokens: int = 400, stride: int = 50) -> List[str]:\n",
    "    if not text.strip():\n",
    "        return []\n",
    "    if prefer_paragraphs:\n",
    "        paras = re.split(r\"\\\\n\\\\s*\\\\n\", text.strip())\n",
    "        paras = [p.strip() for p in paras if p.strip()]\n",
    "        chunks = []\n",
    "        for p in paras:\n",
    "            if len(tokenizer.encode(p, add_special_tokens=False)) <= max_tokens:\n",
    "                chunks.append(p)\n",
    "            else:\n",
    "                chunks.extend(make_chunks_by_tokens(p, tokenizer, max_tokens=max_tokens, stride=stride))\n",
    "        return chunks\n",
    "    else:\n",
    "        return make_chunks_by_tokens(text, tokenizer, max_tokens=max_tokens, stride=stride)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da6aa4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\semde\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# === Alleen de RobBERT v2 sentiment pipeline laden ===\n",
    "ROB_NAME = \"DTAI-KULeuven/robbert-v2-dutch-sentiment\"\n",
    "\n",
    "def load_pipe(name, tries=2):\n",
    "    last = None\n",
    "    for i in range(tries):\n",
    "        try:\n",
    "            clf = pipeline(\n",
    "                task=\"sentiment-analysis\",\n",
    "                model=name,\n",
    "                tokenizer=name,\n",
    "                top_k=None,          # return_all_scores vervangen\n",
    "                truncation=True\n",
    "            )\n",
    "            return clf\n",
    "        except Exception as e:\n",
    "            last = e\n",
    "    raise last\n",
    "\n",
    "rob_pipe = load_pipe(ROB_NAME)\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(ROB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a58e760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAP = {\n",
    "    \"POSITIVE\": \"positief\", \"NEGATIVE\": \"negatief\", \"NEUTRAL\": \"neutraal\",\n",
    "    \"Positive\": \"positief\", \"Negative\": \"negatief\", \"Neutral\": \"neutraal\",\n",
    "    \"positief\": \"positief\", \"negatief\": \"negatief\", \"neutraal\": \"neutraal\"\n",
    "}\n",
    "\n",
    "def normalize_pnn(probs: dict) -> dict:\n",
    "    import numpy as np\n",
    "    arr = np.array([\n",
    "        probs.get(\"positief\", 0.0),\n",
    "        probs.get(\"negatief\", 0.0),\n",
    "        probs.get(\"neutraal\", 0.0)\n",
    "    ], dtype=float)\n",
    "    s = arr.sum()\n",
    "    if s <= 0:\n",
    "        return {\"positief\":0.0, \"negatief\":0.0, \"neutraal\":1.0}\n",
    "    arr = arr / s\n",
    "    return {\"positief\": float(arr[0]), \"negatief\": float(arr[1]), \"neutraal\": float(arr[2])}\n",
    "\n",
    "def score_text_with_pipe(text: str, clf) -> dict:\n",
    "    text = (text or \"\").strip()\n",
    "    if not text:\n",
    "        return {\"positief\":0.0, \"negatief\":0.0, \"neutraal\":1.0}\n",
    "    out = clf(text, truncation=True)\n",
    "    scores = out[0]  # top_k=None -> lijst van dicts\n",
    "    probs = {\"positief\":0.0, \"negatief\":0.0, \"neutraal\":0.0}\n",
    "    for item in scores:\n",
    "        lab = LABEL_MAP.get(item[\"label\"])\n",
    "        if lab:\n",
    "            probs[lab] = float(item[\"score\"])\n",
    "    return normalize_pnn(probs)\n",
    "\n",
    "MAX_MODEL_TOKENS = 512\n",
    "HEADROOM = 8\n",
    "SAFE_MAX = MAX_MODEL_TOKENS - HEADROOM\n",
    "\n",
    "def token_chunks(text, tokenizer, max_tokens=SAFE_MAX, stride=50):\n",
    "    if not text or not text.strip():\n",
    "        return []\n",
    "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    if len(ids) <= max_tokens:\n",
    "        return [text]\n",
    "    out, i = [], 0\n",
    "    while i < len(ids):\n",
    "        j = min(i + max_tokens, len(ids))\n",
    "        chunk_ids = ids[i:j]\n",
    "        out.append(tokenizer.decode(chunk_ids, clean_up_tokenization_spaces=True))\n",
    "        if j >= len(ids): break\n",
    "        i = max(j - stride, i + 1)\n",
    "    return out\n",
    "\n",
    "def aggregate_article_with_pipe_binary(title, lead, body_chunks, clf, tokenizer,\n",
    "                                       weights=None):\n",
    "    \"\"\"\n",
    "    Alleen POS/NEG. Negeert neutraal volledig.\n",
    "    - Titel/lead/body worden gechunked zoals voorheen.\n",
    "    - Weeg t/l/b via weights (default title=2.0, lead=1.0, body=1.0).\n",
    "    - Label = 'positief' als p_pos >= p_neg, anders 'negatief'.\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        from dataclasses import dataclass\n",
    "        @dataclass\n",
    "        class W: title: float = 2.0; lead: float = 1.0; body: float = 1.0\n",
    "        weights = W()\n",
    "\n",
    "    parts = []\n",
    "\n",
    "    # Titel\n",
    "    t_chunks = token_chunks(title or \"\", tokenizer, max_tokens=SAFE_MAX, stride=50)\n",
    "    w_t_each = (weights.title / max(len(t_chunks), 1)) if t_chunks else 0.0\n",
    "    for t in t_chunks:\n",
    "        if t.strip():\n",
    "            parts.append((t.strip(), w_t_each))\n",
    "\n",
    "    # Lead\n",
    "    l_chunks = token_chunks(lead or \"\", tokenizer, max_tokens=SAFE_MAX, stride=50)\n",
    "    w_l_each = (weights.lead / max(len(l_chunks), 1)) if l_chunks else 0.0\n",
    "    for l in l_chunks:\n",
    "        if l.strip():\n",
    "            parts.append((l.strip(), w_l_each))\n",
    "\n",
    "    # Body (al gechunked upstream)\n",
    "    if body_chunks:\n",
    "        w_b_each = weights.body / len(body_chunks)\n",
    "        for ch in body_chunks:\n",
    "            ch = (ch or \"\").strip()\n",
    "            if ch:\n",
    "                parts.append((ch, w_b_each))\n",
    "\n",
    "    if not parts:\n",
    "        # lege tekst -> kies 'negatief' conservatief of geef pos=neg=0.5\n",
    "        return {\"p_pos\":0.5, \"p_neg\":0.5, \"label\":\"negatief\"}\n",
    "\n",
    "    acc_pos = 0.0\n",
    "    acc_neg = 0.0\n",
    "    total_w = 0.0\n",
    "\n",
    "    for txt, w in parts:\n",
    "        out = clf(txt, truncation=True, max_length=MAX_MODEL_TOKENS, padding=False)\n",
    "        scores = out[0]  # lijst met dicts\n",
    "        p_pos = p_neg = 0.0\n",
    "        for item in scores:\n",
    "            lab = LABEL_MAP.get(item[\"label\"])\n",
    "            if lab == \"positief\":\n",
    "                p_pos = float(item[\"score\"])\n",
    "            elif lab == \"negatief\":\n",
    "                p_neg = float(item[\"score\"])\n",
    "            # 'neutraal' negeren we bewust\n",
    "\n",
    "        # Her-normaliseer over POS/NEG alleen (optioneel, maar aan te raden)\n",
    "        s = p_pos + p_neg\n",
    "        if s > 0:\n",
    "            p_pos2 = p_pos / s\n",
    "            p_neg2 = p_neg / s\n",
    "        else:\n",
    "            # als model iets geks geeft, maak gelijk verdeeld\n",
    "            p_pos2 = p_neg2 = 0.5\n",
    "\n",
    "        acc_pos += p_pos2 * w\n",
    "        acc_neg += p_neg2 * w\n",
    "        total_w += w\n",
    "\n",
    "    if total_w <= 0:\n",
    "        total_w = 1.0\n",
    "    p_pos_bin = acc_pos / total_w\n",
    "    p_neg_bin = acc_neg / total_w\n",
    "\n",
    "    label = \"positief\" if p_pos_bin >= p_neg_bin else \"negatief\"\n",
    "    return {\"p_pos\": float(p_pos_bin), \"p_neg\": float(p_neg_bin), \"label\": label}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed84f3d-ba7e-4aea-97dc-23bef9861a26",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44dbc295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PDFs verwerken:   0%|                                                                           | 0/70 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (708 > 512). Running this sequence through the model will result in indexing errors\n",
      "PDFs verwerken: 100%|██████████████████████████████████████████████████████████████████| 70/70 [02:36<00:00,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  id  \\\n",
      "0  Bericht 007 Nederlandse pati_nt wacht te lang ...   \n",
      "1  Bericht 010_Nieuwe kankermedicijnen leveren me...   \n",
      "2  Bericht 011_Hoe controleer je verstopte moeder...   \n",
      "3  Bericht 016_Ik vind het erg als _n infuus van ...   \n",
      "4  Bericht 021_Wachtlijsten en personeelstekort_ ...   \n",
      "\n",
      "                                                file  \\\n",
      "0  Bericht 007 Nederlandse pati_nt wacht te lang ...   \n",
      "1  Bericht 010_Nieuwe kankermedicijnen leveren me...   \n",
      "2  Bericht 011_Hoe controleer je verstopte moeder...   \n",
      "3  Bericht 016_Ik vind het erg als _n infuus van ...   \n",
      "4  Bericht 021_Wachtlijsten en personeelstekort_ ...   \n",
      "\n",
      "                                               title  \\\n",
      "0  Nederlandse patiënt wacht te lang op betere me...   \n",
      "1  Nieuwe kankermedicijnen leveren meer financiël...   \n",
      "2         Hoe controleer je verstopte moedervlekken?   \n",
      "3  'Ik vind het erg als 'n infuus van 25.000 euro...   \n",
      "4  Wachtlijsten en personeelstekort: het 'zorginf...   \n",
      "\n",
      "                                                lead  rob_p_pos  rob_p_neg  \\\n",
      "0  Wat een prachtig bericht onlangs, dat meer kan...   0.360591   0.639409   \n",
      "1  Vorige week verscheen in Trouw een artikel met...   0.337094   0.662906   \n",
      "2  Preventie het consult Meer dan twintig jaar ge...   0.961354   0.038646   \n",
      "3  Slimme ideeën van apotheker Roelof van Leeuwen...   0.256025   0.743975   \n",
      "4  Onbetaalbare zorg De gezondheidszorg kan het w...   0.922337   0.077663   \n",
      "\n",
      "  rob_label  \n",
      "0  negatief  \n",
      "1  negatief  \n",
      "2  positief  \n",
      "3  negatief  \n",
      "4  positief  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "pdf_files = [f for f in sorted(os.listdir(DATA_DIR)) if f.lower().endswith('.pdf')]\n",
    "if not pdf_files:\n",
    "    print(f\"[INFO] Geen PDF-bestanden gevonden in '{DATA_DIR}'.\")\n",
    "else:\n",
    "    for fname in tqdm(pdf_files, desc=\"PDFs verwerken\"):\n",
    "        fpath = os.path.join(DATA_DIR, fname)\n",
    "        clean_txt = pdf_to_text(fpath)\n",
    "        title, lead, body = extract_title_lead_body(clean_txt)\n",
    "\n",
    "        body_chunks = chunk_body(\n",
    "            body,\n",
    "            TOKENIZER,\n",
    "            prefer_paragraphs=True,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            stride=STRIDE\n",
    "        )\n",
    "\n",
    "        rob_bin = aggregate_article_with_pipe_binary(\n",
    "            title, lead, body_chunks,\n",
    "            rob_pipe, TOKENIZER\n",
    "        )\n",
    "\n",
    "        rows.append({\n",
    "            \"id\": extract_id_from_filename(fname) if 'extract_id_from_filename' in globals() else fname,\n",
    "            \"file\": fname,\n",
    "            \"title\": title,\n",
    "            \"lead\": lead,\n",
    "\n",
    "            # Alleen binaire output\n",
    "            \"rob_p_pos\": rob_bin[\"p_pos\"],\n",
    "            \"rob_p_neg\": rob_bin[\"p_neg\"],\n",
    "            \"rob_label\": rob_bin[\"label\"],  # nu altijd 'positief' of 'negatief'\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54bfa0aa-81b3-41b1-9644-ff6947c5bbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Geschreven naar sentiment_results.csv\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"sentiment_results.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"[DONE] Geschreven naar sentiment_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59905cef-9bb6-4aad-beaa-3096691bd2dd",
   "metadata": {},
   "source": [
    "# Classification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed9e99f9-943f-4699-88b8-f8ec5da36dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  robbert_negatief  robbert_positief\n",
      "0   7              True             False\n",
      "1  10              True             False\n",
      "2  11             False              True\n",
      "3  16              True             False\n",
      "4  21             False              True\n"
     ]
    }
   ],
   "source": [
    "# === Load results (RobBERT only) ===\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"sentiment_results.csv\")\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Parse ID (robust)\n",
    "#    - Werkt voor 'Bericht 007_*', 'Bericht_007 ...', of als id al numeriek is\n",
    "# ---------------------------\n",
    "def extract_id(val):\n",
    "    # Als al numeriek:\n",
    "    try:\n",
    "        return int(val)\n",
    "    except Exception:\n",
    "        pass\n",
    "    s = str(val)\n",
    "    m = re.search(r'bericht[_\\s-]*(\\d+)', s, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    # fallback: neem eerste getal dat voorkomt\n",
    "    m2 = re.search(r'(\\d+)', s)\n",
    "    return int(m2.group(1)) if m2 else None\n",
    "\n",
    "if 'id' in df.columns:\n",
    "    df['id'] = df['id'].apply(extract_id)\n",
    "else:\n",
    "    # als er geen 'id' kolom is, maak er één op basis van 'file' of index\n",
    "    if 'file' in df.columns:\n",
    "        df['id'] = df['file'].apply(extract_id)\n",
    "    else:\n",
    "        df['id'] = range(1, len(df) + 1)\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Alleen relevante kolommen (RobBERT)\n",
    "# ---------------------------\n",
    "if 'rob_label' not in df.columns:\n",
    "    raise KeyError(\"Column 'rob_label' not found in sentiment_results.csv\")\n",
    "\n",
    "# labels netjes lowercased (verwacht: positief/negatief/neutraal)\n",
    "df['rob_label'] = df['rob_label'].astype(str).str.lower()\n",
    "\n",
    "df_labels = df[['id', 'rob_label']].copy()\n",
    "\n",
    "# ---------------------------\n",
    "# 3) One-hot encoding voor RobBERT\n",
    "#    >>> levert kolommen: robbert_positief, robbert_negatief, robbert_neutraal\n",
    "# ---------------------------\n",
    "df_encoded = pd.get_dummies(df_labels, columns=['rob_label'], prefix='robbert')\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Bekijken\n",
    "# ---------------------------\n",
    "print(df_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c721f36c-0e73-4002-84ca-5c44c22d02ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id  robbert_negatief  robbert_positief\n",
      "65  287             False              True\n",
      "66  296             False              True\n",
      "67  300              True             False\n",
      "68  306             False              True\n",
      "69   55             False              True\n"
     ]
    }
   ],
   "source": [
    "print(df_encoded.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b13baf7e-c8be-48b8-b3d3-525f660a34fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         positief  negatief\n",
      "Model                      \n",
      "robbert        52        18\n"
     ]
    }
   ],
   "source": [
    "summary = {}\n",
    "\n",
    "for model_prefix in ['robbert']:\n",
    "    summary[model_prefix] = {\n",
    "        'positief': df_encoded.filter(like=f\"{model_prefix}_positief\").sum().values[0],\n",
    "        #'neutraal': df_encoded.filter(like=f\"{model_prefix}_neutraal\").sum().values[0],\n",
    "        'negatief': df_encoded.filter(like=f\"{model_prefix}_negatief\").sum().values[0]\n",
    "    }\n",
    "\n",
    "# Zet om naar DataFrame\n",
    "summary_df = pd.DataFrame(summary).T\n",
    "summary_df.index.name = \"Model\"\n",
    "\n",
    "# Resultaat bekijken\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bbfb9f-bdec-41ac-9baa-f0fd14caf124",
   "metadata": {},
   "source": [
    "# Analyse tov human sentiment (maaike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cae47f8e-d53d-46f7-868e-2eb9ced9ceea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RobBERT (binaire evaluatie) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positief      0.745     0.875     0.805        40\n",
      "    negatief      0.722     0.520     0.605        25\n",
      "\n",
      "    accuracy                          0.738        65\n",
      "   macro avg      0.733     0.698     0.705        65\n",
      "weighted avg      0.736     0.738     0.728        65\n",
      "\n",
      "Confusion matrix:\n",
      "               Pred positief  Pred negatief\n",
      "True positief             35              5\n",
      "True negatief             12             13\n"
     ]
    }
   ],
   "source": [
    "# === Evaluatie: Human vs RobBERT (binaire evaluatie: positief vs negatief) ===\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1) Data inladen\n",
    "df_h = pd.read_excel(\"Human_Sentiment.xlsx\")   # kolommen: Artikel, Sentiment\n",
    "df_n = pd.read_csv(\"sentiment_results.csv\")    # bevat 'id' + rob_label (pos/neg)\n",
    "\n",
    "# 2) ID-parsing\n",
    "def extract_id(s):\n",
    "    s = str(s)\n",
    "    m = re.search(r'bericht[_\\s-]*(\\d+)', s, flags=re.IGNORECASE)\n",
    "    if m: return int(m.group(1))\n",
    "    m2 = re.search(r'(\\d+)', s)\n",
    "    return int(m2.group(1)) if m2 else None\n",
    "\n",
    "if 'id' in df_n.columns:\n",
    "    df_n['id'] = df_n['id'].apply(extract_id)\n",
    "elif 'file' in df_n.columns:\n",
    "    df_n['id'] = df_n['file'].apply(extract_id)\n",
    "else:\n",
    "    df_n['id'] = range(1, len(df_n) + 1)\n",
    "\n",
    "if not pd.api.types.is_integer_dtype(df_h['Artikel']):\n",
    "    try:\n",
    "        df_h['Artikel'] = df_h['Artikel'].astype(int)\n",
    "    except Exception:\n",
    "        df_h['Artikel'] = df_h['Artikel'].apply(extract_id)\n",
    "\n",
    "# 3) Human labels normaliseren en filteren (alleen pos/neg)\n",
    "df_h['Human_Label'] = (\n",
    "    df_h['Sentiment']\n",
    "      .astype(str).str.strip().str.lower()\n",
    ")\n",
    "# Drop 'mixed' en 'neutral' (vallen af)\n",
    "df_h = df_h[df_h['Human_Label'].isin({'positief','negatief'})].copy()\n",
    "\n",
    "# 4) Merge\n",
    "dfm = pd.merge(df_h, df_n, left_on='Artikel', right_on='id', how='inner')\n",
    "\n",
    "# 5) RobBERT kolom\n",
    "if 'rob_label' not in dfm.columns:\n",
    "    raise KeyError(\"rob_label niet gevonden in sentiment_results.csv (verwacht binaire output).\")\n",
    "y_true = dfm['Human_Label'].astype(str).str.lower()\n",
    "y_pred = dfm['rob_label'].astype(str).str.lower()\n",
    "\n",
    "# 6) Evaluatie (binaire labels)\n",
    "labels_order = ['positief','negatief']\n",
    "print(\"\\n=== RobBERT (binaire evaluatie) ===\")\n",
    "print(classification_report(\n",
    "    y_true, y_pred,\n",
    "    labels=labels_order,\n",
    "    target_names=labels_order,\n",
    "    digits=3\n",
    "))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels_order)\n",
    "cm_df = pd.DataFrame(\n",
    "    cm,\n",
    "    index=[f\"True {l}\" for l in labels_order],\n",
    "    columns=[f\"Pred {l}\" for l in labels_order]\n",
    ")\n",
    "print(\"Confusion matrix:\")\n",
    "print(cm_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9e2095b-fb45-4d10-a6e0-8216e93efa7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " id                                                                                                      title rob_label Human_Label\n",
      "  7                                        Nederlandse patiënt wacht te lang op betere medicijnen tegen kanker  negatief    positief\n",
      " 16                                           'Ik vind het erg als 'n infuus van 25.000 euro wordt weggegooid'  negatief    positief\n",
      " 26                          Tijd om te kiezen: dure behandelingen of voldoende zorg voor ouderen ; Commentaar  positief    negatief\n",
      " 63                                                                          Langer lijden of waardig sterven?  positief    negatief\n",
      " 66                                                               'Mijn belangrijkste vraag: wat wil je echt?'  positief    negatief\n",
      " 69                                                       Geef nieuwe medicijnen geen groen licht, maar oranje  positief    negatief\n",
      " 74                                         Goed dat grens wordt gesteld aan uitdijen basispakket ; Commentaar  positief    negatief\n",
      " 86                                                                     Lat omhoog bij nieuwe kankermedicijnen  positief    negatief\n",
      " 94                              Toezichthouder moet strenger omgaan met kankermedicijnen die valse hoop geven  positief    negatief\n",
      " 99                      Harde keuzes zijn nodig in de zorg voordat nog meer mensen de dupe worden; Commentaar  positief    negatief\n",
      "120   Zijn die betere kankermedicijnen al dat geld waard?; Geneeskunde Zijn die betere kankermedicijnen al dat  positief    negatief\n",
      "168                                                             Hoe dodelijk is kanker? Is de ziekte erfelijk?  positief    negatief\n",
      "181                                                                     'Ik wilde geen Albert Schweitzer zijn'  positief    negatief\n",
      "182                                                                       Soms loont even afwachten bij kanker  negatief    positief\n",
      "248                                                                  Kunnen we onze medicijnen wel vertrouwen?  positief    negatief\n",
      "271                                            Beschikbaarheid van dure, levensreddende medicatie is in gevaar  negatief    positief\n",
      "300 'Doodgaan is geen optie'; behandeling zou leven van topatleet Jeroen Reesen (39) kunnen redden, maar wordt  negatief    positief\n"
     ]
    }
   ],
   "source": [
    "errors = dfm[(dfm['Human_Label'] != dfm['rob_label'])]\n",
    "print(errors[['id', 'title', 'rob_label', 'Human_Label']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282532a9-c576-4869-88cc-c0f16bfd962e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
