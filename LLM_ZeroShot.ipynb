{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d76373ac-b786-47f8-bfa9-1c246940f7c1",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd83ed5-b686-4605-99fe-91be0c7e8fb1",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1787f3b8-ad2e-4fb2-8a09-d84666879bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, re, json\n",
    "from dataclasses import dataclass\n",
    "from typing import Literal, Optional, Dict, Any, List\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import InferenceClient\n",
    "from groq import Groq\n",
    "from openai import OpenAI as OpenAIClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dcef817-3af7-47e4-883b-402133d7c8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# API keys uit .env\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "GROQ_API_KEY   = os.getenv(\"GROQ_API_KEY\", \"\")\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\", \"\")\n",
    "\n",
    "# (Optioneel) dedicated HF endpoint URLs (laat leeg om Hosted Inference API te gebruiken)\n",
    "HF_ENDPOINT_URL_DUTCH = os.getenv(\"HF_ENDPOINT_URL_DUTCH\", \"\").strip()\n",
    "HF_ENDPOINT_URL_EN    = \"\"\n",
    "\n",
    "# Label normalisatie\n",
    "CANONICAL = {\"positief\":\"positive\",\"negatief\":\"negative\",\"positive\":\"positive\",\"negative\":\"negative\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dca3e2cd-c953-40de-9b5f-560f6970a7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_label(text: str) -> Optional[str]:\n",
    "    if not text:\n",
    "        return None\n",
    "    t = text.strip().lower()\n",
    "    if re.search(r\"\\bpos(itive|itief)?\\b\", t): return \"positive\"\n",
    "    if re.search(r\"\\bneg(ative|atief)?\\b\", t): return \"negative\"\n",
    "    for k,v in CANONICAL.items():\n",
    "        if k in t: return v\n",
    "    return None\n",
    "\n",
    "def to_score(label: Optional[str]) -> Optional[int]:\n",
    "    return {\"positive\": 1, \"negative\": -1}.get(label, None)\n",
    "\n",
    "def safe_colname(name: str) -> str:\n",
    "    return re.sub(r\"[^A-Za-z0-9_]+\", \"_\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cd9e2c1-7d12-43c2-a0f2-c7fd82cc3910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                         title_text  \\\n",
      "0   7  Nederlandse patiënt wacht te lang op betere me...   \n",
      "1  10  Nieuwe kankermedicijnen leveren meer financiël...   \n",
      "2  11         Hoe controleer je verstopte moedervlekken?   \n",
      "\n",
      "                                           lead_text  \\\n",
      "0  Wat een prachtig bericht onlangs, dat meer kan...   \n",
      "1  Vorige week verscheen in Trouw een artikel met...   \n",
      "2  Meer dan twintig jaar geleden ontdekte ze op h...   \n",
      "\n",
      "                                           body_text  \n",
      "0  Maar het is jammer dat het zo lang duurt voord...  \n",
      "1  Nederland is een mooi land waarin uiteindelijk...  \n",
      "2  Eerst over die dagelijkse inspectie. Dat is ec...  \n",
      "Loaded 70 rows from out/Title_Lead_Body.xlsx\n"
     ]
    }
   ],
   "source": [
    "XLSX_PATH = \"out/Title_Lead_Body.xlsx\"\n",
    "\n",
    "df_input = pd.read_excel(XLSX_PATH)\n",
    "df_input = df_input.rename(columns={c: c.lower() for c in df_input.columns})\n",
    "\n",
    "need = {\"id\",\"title\",\"lead\",\"body\"}\n",
    "missing = need - set(df_input.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Ontbrekende kolommen in {XLSX_PATH}: {missing}\")\n",
    "\n",
    "df_input[\"id\"] = df_input[\"id\"].astype(int)\n",
    "\n",
    "def _to_str(x):\n",
    "    return \"\" if pd.isna(x) else str(x)\n",
    "\n",
    "df = df_input.copy()\n",
    "df[\"title_text\"] = df[\"title\"].apply(_to_str)\n",
    "df[\"lead_text\"]  = df[\"lead\"].apply(_to_str)\n",
    "df[\"body_text\"]  = df[\"body\"].apply(_to_str)  # body al geaggregeerd tot 1 tekstveld\n",
    "\n",
    "# Volledig lege rijen verwijderen\n",
    "df = df[~(df[\"title_text\"].eq(\"\") & df[\"lead_text\"].eq(\"\") & df[\"body_text\"].eq(\"\"))].reset_index(drop=True)\n",
    "\n",
    "print(df[[\"id\",\"title_text\",\"lead_text\",\"body_text\"]].head(3))\n",
    "print(f\"Loaded {len(df)} rows from {XLSX_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4d81e28-2bcc-4fb6-9f51-e34ab440ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%python\n",
    "SYSTEM_NL = (\n",
    "    \"Je bent een sentimentclassifier. Geef uitsluitend 'positief' of 'negatief' terug, zonder extra uitleg.\"\n",
    ")\n",
    "\n",
    "def make_inst_prompt(content: str) -> str:\n",
    "    # Altijd Nederlands, ook voor “Engelse” modellen (multilingual werkt prima)\n",
    "    return (\n",
    "        'Generative LLMs \"\"\"[INST] <<SYS>>\\n'\n",
    "        f\"{SYSTEM_NL}\\n\"\n",
    "        \"<</SYS>>\\n\"\n",
    "        \"Is het sentiment in het volgende Nederlandstalige krantenartikel\\n\"\n",
    "        \"positief of negatief?\\n\"\n",
    "        f\"{content} [/INST] \\\"\\\"\\\"\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c8a4216-0963-4fba-933e-8c354ced2c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- clients ---\n",
    "hf_client_nl = InferenceClient(\n",
    "    model=HF_ENDPOINT_URL_DUTCH if HF_ENDPOINT_URL_DUTCH else \"BramVanroy/Llama-2-13b-chat-dutch\",\n",
    "    api_key=HUGGINGFACE_API_KEY,\n",
    ")\n",
    "\n",
    "hf_client_en = InferenceClient(\n",
    "    model=\"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    token=HUGGINGFACE_API_KEY\n",
    ")\n",
    "\n",
    "groq_client   = Groq(api_key=GROQ_API_KEY)            if GROQ_API_KEY   else None\n",
    "openai_client = OpenAIClient(api_key=OPENAI_API_KEY)  if OPENAI_API_KEY else None\n",
    "\n",
    "@dataclass\n",
    "class ModelSpec:\n",
    "    name: str\n",
    "    provider: Literal[\"hf_api\",\"groq\",\"openai\"]\n",
    "    model_id: str   # informatief (logging)\n",
    "    language: Literal[\"nl\"]  # we gebruiken overal NL prompt\n",
    "\n",
    "def _hf_chat_completion(client, model_id: str, prompt: str, temperature: float, max_tokens: int) -> str:\n",
    "    comp = client.chat.completions.create(\n",
    "        model=model_id,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return comp.choices[0].message.content.strip()\n",
    "\n",
    "def _hf_text_gen(client, prompt: str, temperature: float, max_tokens: int) -> str:\n",
    "    return client.text_generation(\n",
    "        prompt,\n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        do_sample=temperature > 0,\n",
    "        return_full_text=False,\n",
    "    ).strip()\n",
    "\n",
    "# ==== NIEUW: HF 70B client ====\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "hf_client_70b = InferenceClient(\n",
    "    model=\"meta-llama/Meta-Llama-3-70B\",\n",
    "    api_key=HUGGINGFACE_API_KEY,\n",
    ")\n",
    "\n",
    "def call_model(spec: ModelSpec, text: str, temperature: float=0.0, max_tokens:int=3) -> str:\n",
    "    \"\"\"Stuurt de NL [INST]-prompt naar de juiste provider en retourneert rauwe output.\"\"\"\n",
    "    prompt = make_inst_prompt(text)\n",
    "\n",
    "    if spec.provider == \"hf_api\":\n",
    "        # Kies juiste HF client o.b.v. model_id\n",
    "        mid = spec.model_id.lower()\n",
    "        if \"bramvanroy/llama-2-13b-chat-dutch\" in mid:\n",
    "            client = hf_client_nl\n",
    "        elif \"meta-llama/meta-llama-3-70b\" in mid or \"meta-llama-3-70b\" in mid:\n",
    "            client = hf_client_70b\n",
    "        else:\n",
    "            client = hf_client_en\n",
    "\n",
    "        if client is None:\n",
    "            return \"[error_hf:client_none] geen HF client beschikbaar\"\n",
    "\n",
    "        # Probeer text-generation; als provider 'conversational' vereist, schakel over naar chat\n",
    "        try:\n",
    "            return _hf_text_gen(client, prompt, temperature, max_tokens)\n",
    "        except Exception as e:\n",
    "            msg = str(e)\n",
    "            if \"Supported task: conversational\" in msg or \"not supported for task text-generation\" in msg:\n",
    "                try:\n",
    "                    return _hf_chat_completion(client, spec.model_id, prompt, temperature, max_tokens)\n",
    "                except Exception as e2:\n",
    "                    return f\"[error_hf_chat:{type(e2).__name__}] {e2}\"\n",
    "            return f\"[error_hf:{type(e).__name__}] {e}\"\n",
    "\n",
    "    if spec.provider == \"groq\":\n",
    "        assert groq_client, \"GROQ_API_KEY ontbreekt in .env\"\n",
    "        resp = groq_client.chat.completions.create(\n",
    "            model=spec.model_id,\n",
    "            messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        return resp.choices[0].message.content.strip()\n",
    "\n",
    "    if spec.provider == \"openai\":\n",
    "        assert openai_client, \"OPENAI_API_KEY ontbreekt in .env\"\n",
    "        resp = openai_client.chat.completions.create(\n",
    "            model=spec.model_id,\n",
    "            messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        return resp.choices[0].message.content.strip()\n",
    "\n",
    "    raise ValueError(f\"Unknown provider: {spec.provider}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "600cb57a-f0ef-4d79-9649-0f938d4c0a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModelSpec(name='HF-Llama-2-13B-dutch', provider='hf_api', model_id='BramVanroy/Llama-2-13b-chat-dutch', language='nl'),\n",
       " ModelSpec(name='HF-Llama-2-13B-en', provider='hf_api', model_id='meta-llama/Llama-2-13b-chat-hf', language='nl'),\n",
       " ModelSpec(name='HF-Llama-3-70B', provider='hf_api', model_id='meta-llama/Meta-Llama-3-70B', language='nl'),\n",
       " ModelSpec(name='GPT-4.0', provider='openai', model_id='gpt-4o', language='nl')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%python\n",
    "MODELS = [\n",
    "    # HF — BramVanroy NL\n",
    "    ModelSpec(\n",
    "        name=\"HF-Llama-2-13B-dutch\",\n",
    "        provider=\"hf_api\",\n",
    "        model_id=\"BramVanroy/Llama-2-13b-chat-dutch\",\n",
    "        language=\"nl\",\n",
    "    ),\n",
    "    # HF — Llama-2 EN (krijgt NL prompt, schakelt naar conversational indien nodig)\n",
    "    ModelSpec(\n",
    "        name=\"HF-Llama-2-13B-en\",\n",
    "        provider=\"hf_api\",\n",
    "        model_id=\"meta-llama/Llama-2-13b-chat-hf\",\n",
    "        language=\"nl\",\n",
    "    ),\n",
    "    # HF — Meta-Llama-3-70B via Inference provider\n",
    "    ModelSpec(\n",
    "        name=\"HF-Llama-3-70B\",\n",
    "        provider=\"hf_api\",\n",
    "        model_id=\"meta-llama/Meta-Llama-3-70B\",\n",
    "        language=\"nl\",\n",
    "    ),\n",
    "    # OpenAI — GPT-4.0\n",
    "    ModelSpec(\n",
    "        name=\"GPT-4.0\",\n",
    "        provider=\"openai\",\n",
    "        model_id=\"gpt-4o\",\n",
    "        language=\"nl\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14d02d23-63a9-4e29-b8ce-216b71e436a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%python\n",
    "def classify_text(spec: ModelSpec, text: str) -> Dict[str, Any]:\n",
    "    if not text or not text.strip():\n",
    "        return {\"label\": None, \"score\": None, \"raw\": \"\"}\n",
    "    raw = call_model(spec, text, temperature=0.0, max_tokens=3)\n",
    "    label = normalize_label(raw)\n",
    "    score = to_score(label)\n",
    "    return {\"label\": label, \"score\": score, \"raw\": raw}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d2ed91ab-8eb3-4904-9530-3a26bbf2f528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Running HF-Llama-2-13B-dutch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:27<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Running HF-Llama-2-13B-en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:27<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Running HF-Llama-3-70B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [00:28<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Running GPT-4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [02:51<00:00,  2.45s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title_text</th>\n",
       "      <th>lead_text</th>\n",
       "      <th>body_text</th>\n",
       "      <th>HF_Llama_2_13B_dutch_title_label</th>\n",
       "      <th>HF_Llama_2_13B_dutch_title_score</th>\n",
       "      <th>HF_Llama_2_13B_dutch_lead_label</th>\n",
       "      <th>HF_Llama_2_13B_dutch_lead_score</th>\n",
       "      <th>HF_Llama_2_13B_dutch_body_label</th>\n",
       "      <th>HF_Llama_2_13B_dutch_body_score</th>\n",
       "      <th>...</th>\n",
       "      <th>HF_Llama_3_70B_lead_label</th>\n",
       "      <th>HF_Llama_3_70B_lead_score</th>\n",
       "      <th>HF_Llama_3_70B_body_label</th>\n",
       "      <th>HF_Llama_3_70B_body_score</th>\n",
       "      <th>GPT_4_0_title_label</th>\n",
       "      <th>GPT_4_0_title_score</th>\n",
       "      <th>GPT_4_0_lead_label</th>\n",
       "      <th>GPT_4_0_lead_score</th>\n",
       "      <th>GPT_4_0_body_label</th>\n",
       "      <th>GPT_4_0_body_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Nederlandse patiënt wacht te lang op betere me...</td>\n",
       "      <td>Wat een prachtig bericht onlangs, dat meer kan...</td>\n",
       "      <td>Maar het is jammer dat het zo lang duurt voord...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Nieuwe kankermedicijnen leveren meer financiël...</td>\n",
       "      <td>Vorige week verscheen in Trouw een artikel met...</td>\n",
       "      <td>Nederland is een mooi land waarin uiteindelijk...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>Hoe controleer je verstopte moedervlekken?</td>\n",
       "      <td>Meer dan twintig jaar geleden ontdekte ze op h...</td>\n",
       "      <td>Eerst over die dagelijkse inspectie. Dat is ec...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>'Ik vind het erg als 'n infuus van 25.000 euro...</td>\n",
       "      <td>Waarom schrijven artsen 1005 milligram van een...</td>\n",
       "      <td>Ziekenhuisapotheker Roelof van Leeuwen zet zic...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>Wachtlijsten en personeelstekort: het 'zorginf...</td>\n",
       "      <td>De gezondheidszorg is 'op', er zit geen rek me...</td>\n",
       "      <td>Verpleegkundigen, verzorgenden, huisartsen, sp...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                         title_text  \\\n",
       "0   7  Nederlandse patiënt wacht te lang op betere me...   \n",
       "1  10  Nieuwe kankermedicijnen leveren meer financiël...   \n",
       "2  11         Hoe controleer je verstopte moedervlekken?   \n",
       "3  16  'Ik vind het erg als 'n infuus van 25.000 euro...   \n",
       "4  21  Wachtlijsten en personeelstekort: het 'zorginf...   \n",
       "\n",
       "                                           lead_text  \\\n",
       "0  Wat een prachtig bericht onlangs, dat meer kan...   \n",
       "1  Vorige week verscheen in Trouw een artikel met...   \n",
       "2  Meer dan twintig jaar geleden ontdekte ze op h...   \n",
       "3  Waarom schrijven artsen 1005 milligram van een...   \n",
       "4  De gezondheidszorg is 'op', er zit geen rek me...   \n",
       "\n",
       "                                           body_text  \\\n",
       "0  Maar het is jammer dat het zo lang duurt voord...   \n",
       "1  Nederland is een mooi land waarin uiteindelijk...   \n",
       "2  Eerst over die dagelijkse inspectie. Dat is ec...   \n",
       "3  Ziekenhuisapotheker Roelof van Leeuwen zet zic...   \n",
       "4  Verpleegkundigen, verzorgenden, huisartsen, sp...   \n",
       "\n",
       "  HF_Llama_2_13B_dutch_title_label  HF_Llama_2_13B_dutch_title_score  \\\n",
       "0                         positive                                 1   \n",
       "1                         positive                                 1   \n",
       "2                         positive                                 1   \n",
       "3                         positive                                 1   \n",
       "4                         positive                                 1   \n",
       "\n",
       "  HF_Llama_2_13B_dutch_lead_label  HF_Llama_2_13B_dutch_lead_score  \\\n",
       "0                        positive                                1   \n",
       "1                        positive                                1   \n",
       "2                        positive                                1   \n",
       "3                        positive                                1   \n",
       "4                        positive                                1   \n",
       "\n",
       "  HF_Llama_2_13B_dutch_body_label  HF_Llama_2_13B_dutch_body_score  ...  \\\n",
       "0                        positive                                1  ...   \n",
       "1                        positive                                1  ...   \n",
       "2                        positive                                1  ...   \n",
       "3                        positive                                1  ...   \n",
       "4                        positive                                1  ...   \n",
       "\n",
       "  HF_Llama_3_70B_lead_label HF_Llama_3_70B_lead_score  \\\n",
       "0                      None                      None   \n",
       "1                      None                      None   \n",
       "2                      None                      None   \n",
       "3                      None                      None   \n",
       "4                      None                      None   \n",
       "\n",
       "  HF_Llama_3_70B_body_label HF_Llama_3_70B_body_score GPT_4_0_title_label  \\\n",
       "0                      None                      None            negative   \n",
       "1                      None                      None            negative   \n",
       "2                      None                      None            negative   \n",
       "3                      None                      None            negative   \n",
       "4                      None                      None            negative   \n",
       "\n",
       "  GPT_4_0_title_score GPT_4_0_lead_label GPT_4_0_lead_score  \\\n",
       "0                  -1           positive                  1   \n",
       "1                  -1           negative                 -1   \n",
       "2                  -1           negative                 -1   \n",
       "3                  -1           negative                 -1   \n",
       "4                  -1           negative                 -1   \n",
       "\n",
       "  GPT_4_0_body_label GPT_4_0_body_score  \n",
       "0           negative                 -1  \n",
       "1           negative                 -1  \n",
       "2           negative                 -1  \n",
       "3           positive                  1  \n",
       "4           negative                 -1  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%python\n",
    "results = df[[\"id\",\"title_text\",\"lead_text\",\"body_text\"]].copy()\n",
    "\n",
    "for spec in MODELS:\n",
    "    print(f\"==> Running {spec.name}\")\n",
    "    base = safe_colname(spec.name)\n",
    "    tlab, tscore = [], []\n",
    "    llab, lscore = [], []\n",
    "    blab, bscore = [], []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        r_title = classify_text(spec, row[\"title_text\"])\n",
    "        r_lead  = classify_text(spec, row[\"lead_text\"])\n",
    "        r_body  = classify_text(spec, row[\"body_text\"])\n",
    "\n",
    "        tlab.append(r_title[\"label\"]); tscore.append(r_title[\"score\"])\n",
    "        llab.append(r_lead[\"label\"]);  lscore.append(r_lead[\"score\"])\n",
    "        blab.append(r_body[\"label\"]);  bscore.append(r_body[\"score\"])\n",
    "\n",
    "    results[f\"{base}_title_label\"] = tlab\n",
    "    results[f\"{base}_title_score\"] = tscore\n",
    "    results[f\"{base}_lead_label\"]  = llab\n",
    "    results[f\"{base}_lead_score\"]  = lscore\n",
    "    results[f\"{base}_body_label\"]  = blab\n",
    "    results[f\"{base}_body_score\"]  = bscore\n",
    "\n",
    "results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "88c35407-a123-4ec1-8bd9-7d08f355b82d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HF_Llama_2_13B_dutch_overall_score</th>\n",
       "      <th>HF_Llama_2_13B_dutch_overall_label</th>\n",
       "      <th>HF_Llama_2_13B_en_overall_score</th>\n",
       "      <th>HF_Llama_2_13B_en_overall_label</th>\n",
       "      <th>HF_Llama_3_70B_overall_score</th>\n",
       "      <th>HF_Llama_3_70B_overall_label</th>\n",
       "      <th>GPT_4_0_overall_score</th>\n",
       "      <th>GPT_4_0_overall_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HF_Llama_2_13B_dutch_overall_score HF_Llama_2_13B_dutch_overall_label  \\\n",
       "0                                   1                           positive   \n",
       "1                                   1                           positive   \n",
       "2                                   1                           positive   \n",
       "3                                   1                           positive   \n",
       "4                                   1                           positive   \n",
       "\n",
       "  HF_Llama_2_13B_en_overall_score HF_Llama_2_13B_en_overall_label  \\\n",
       "0                            None                             NaN   \n",
       "1                            None                             NaN   \n",
       "2                            None                             NaN   \n",
       "3                            None                             NaN   \n",
       "4                            None                             NaN   \n",
       "\n",
       "  HF_Llama_3_70B_overall_score HF_Llama_3_70B_overall_label  \\\n",
       "0                         None                          NaN   \n",
       "1                         None                          NaN   \n",
       "2                         None                          NaN   \n",
       "3                         None                          NaN   \n",
       "4                         None                          NaN   \n",
       "\n",
       "   GPT_4_0_overall_score GPT_4_0_overall_label  \n",
       "0                     -1              negative  \n",
       "1                     -1              negative  \n",
       "2                     -1              negative  \n",
       "3                     -1              negative  \n",
       "4                     -1              negative  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%python\n",
    "def combine_scores(row, base:str) -> Optional[int]:\n",
    "    scores = [row.get(f\"{base}_title_score\"), row.get(f\"{base}_lead_score\"), row.get(f\"{base}_body_score\")]\n",
    "    scores = [s for s in scores if s in (-1, 1)]\n",
    "    if not scores:\n",
    "        return None\n",
    "    s = sum(scores)\n",
    "    if s > 0: return 1\n",
    "    if s < 0: return -1\n",
    "    return 0\n",
    "\n",
    "for spec in MODELS:\n",
    "    base = safe_colname(spec.name)\n",
    "    results[f\"{base}_overall_score\"] = results.apply(lambda r: combine_scores(r, base), axis=1)\n",
    "    results[f\"{base}_overall_label\"] = results[f\"{base}_overall_score\"].map({1:\"positive\",-1:\"negative\",0:\"neutral\"})\n",
    "\n",
    "results.filter(regex=\"overall_\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "727cacf4-e25c-4f71-9d30-adab9d56dde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved -> out/newspapers_sentiment_llms.xlsx\n"
     ]
    }
   ],
   "source": [
    "# %%python\n",
    "OUT_XLSX = \"out/newspapers_sentiment_llms.xlsx\"\n",
    "with pd.ExcelWriter(OUT_XLSX, engine=\"openpyxl\") as xw:\n",
    "    results.to_excel(xw, index=False, sheet_name=\"sentiment\")\n",
    "print(f\"Saved -> {OUT_XLSX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "beeca903-8e21-4fa9-bf74-d7d6fec1279a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>HF_Llama_2_13B_dutch_overall_label</th>\n",
       "      <th>HF_Llama_2_13B_en_overall_label</th>\n",
       "      <th>HF_Llama_3_70B_overall_label</th>\n",
       "      <th>GPT_4_0_overall_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>positive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>positive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>positive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>positive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>positive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>26</td>\n",
       "      <td>positive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>28</td>\n",
       "      <td>positive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>30</td>\n",
       "      <td>positive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>35</td>\n",
       "      <td>positive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>37</td>\n",
       "      <td>positive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id HF_Llama_2_13B_dutch_overall_label HF_Llama_2_13B_en_overall_label  \\\n",
       "0   7                           positive                             NaN   \n",
       "1  10                           positive                             NaN   \n",
       "2  11                           positive                             NaN   \n",
       "3  16                           positive                             NaN   \n",
       "4  21                           positive                             NaN   \n",
       "5  26                           positive                             NaN   \n",
       "6  28                           positive                             NaN   \n",
       "7  30                           positive                             NaN   \n",
       "8  35                           positive                             NaN   \n",
       "9  37                           positive                             NaN   \n",
       "\n",
       "  HF_Llama_3_70B_overall_label GPT_4_0_overall_label  \n",
       "0                          NaN              negative  \n",
       "1                          NaN              negative  \n",
       "2                          NaN              negative  \n",
       "3                          NaN              negative  \n",
       "4                          NaN              negative  \n",
       "5                          NaN              negative  \n",
       "6                          NaN              positive  \n",
       "7                          NaN              positive  \n",
       "8                          NaN              negative  \n",
       "9                          NaN              positive  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HF-Llama-2-13B-dutch ===\n",
      "HF_Llama_2_13B_dutch_overall_label\n",
      "positive    70\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== HF-Llama-2-13B-en ===\n",
      "HF_Llama_2_13B_en_overall_label\n",
      "NaN    70\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== HF-Llama-3-70B ===\n",
      "HF_Llama_3_70B_overall_label\n",
      "NaN    70\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== GPT-4.0 ===\n",
      "GPT_4_0_overall_label\n",
      "negative    40\n",
      "positive    30\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# %%python\n",
    "summary_cols = []\n",
    "for spec in MODELS:\n",
    "    base = safe_colname(spec.name)\n",
    "    summary_cols.append(f\"{base}_overall_label\")\n",
    "\n",
    "display(results[[\"id\"] + summary_cols].head(10))\n",
    "\n",
    "for spec in MODELS:\n",
    "    base = safe_colname(spec.name)\n",
    "    print(f\"\\n=== {spec.name} ===\")\n",
    "    print(results[f\"{base}_overall_label\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b423e460-6ab7-4be9-ba83-6a5b473dcb57",
   "metadata": {},
   "source": [
    "NA values due to free rate limits reached, will start an endpoint to get the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a460217-88c6-42c8-ae4b-a178866242bc",
   "metadata": {},
   "source": [
    "# Retry Llama 13B EN and Llama 70B due to rate limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ce3991f-aa46-48c0-9a41-0ef7800b61ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\") or os.getenv(\"HF_TOKEN\") or \"\"\n",
    "\n",
    "# Endpoints (moeten in .env staan)\n",
    "#HF_ENDPOINT_URL_13B      = os.getenv(\"HF_ENDPOINT_URL_13B\", \"\").strip()        # Llama-2-13B (EN) endpoint\n",
    "#HF_ENDPOINT_URL_70B      = os.getenv(\"HF_ENDPOINT_URL_70B\", \"\").strip()        # Llama-3-70B endpoint\n",
    "HF_ENDPOINT_URL_8B_DUTCH = os.getenv(\"HF_ENDPOINT_URL_8B_DUTCH\", \"\").strip()   # ReBatch/Llama-3-8B-dutch endpoint\n",
    "HF_ENDPOINT_URL_8B_EN    = os.getenv(\"HF_ENDPOINT_URL_8B_EN\", \"\").strip()      # Meta-Llama-3-8B endpoint\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\", \"\")\n",
    "\n",
    "assert HUGGINGFACE_API_KEY, \"HUGGINGFACE_API_KEY/HF_TOKEN ontbreekt in .env\"\n",
    "#assert HF_ENDPOINT_URL_13B, \"HF_ENDPOINT_URL_13B ontbreekt in .env\"\n",
    "#assert HF_ENDPOINT_URL_70B, \"HF_ENDPOINT_URL_70B ontbreekt in .env\"\n",
    "assert HF_ENDPOINT_URL_8B_DUTCH, \"HF_ENDPOINT_URL_8B_DUTCH ontbreekt in .env\"\n",
    "assert HF_ENDPOINT_URL_8B_EN, \"HF_ENDPOINT_URL_8B_EN ontbreekt in .env\"\n",
    "assert GROQ_API_KEY, \"GROQ_API_KEY ontbreekt in .env\"\n",
    "\n",
    "# HF clients: gebruik jouw dedicated endpoints\n",
    "#hf_client_en       = InferenceClient(model=HF_ENDPOINT_URL_13B,      api_key=HUGGINGFACE_API_KEY)  # Llama-2-13B (EN)\n",
    "#hf_client_70b      = InferenceClient(model=HF_ENDPOINT_URL_70B,      api_key=HUGGINGFACE_API_KEY)  # Llama-3-70B\n",
    "hf_client_8b_dutch = InferenceClient(model=HF_ENDPOINT_URL_8B_DUTCH, api_key=HUGGINGFACE_API_KEY)  # ReBatch L3-8B Dutch\n",
    "hf_client_8b_en    = InferenceClient(model=HF_ENDPOINT_URL_8B_EN,    api_key=HUGGINGFACE_API_KEY)  # Meta-Llama-3-8B (EN)\n",
    "groq_client = Groq(api_key=GROQ_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a115082c-dcf9-487d-845f-cf00ba00f0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _hf_chat_completion(client, prompt: str, temperature: float, max_tokens: int) -> str:\n",
    "    comp = client.chat.completions.create(\n",
    "        model=client.model,  # endpoint zelf bepaalt het onderliggende model\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return comp.choices[0].message.content.strip()\n",
    "\n",
    "def _hf_text_gen(client, prompt: str, temperature: float, max_tokens: int) -> str:\n",
    "    return client.text_generation(\n",
    "        prompt,\n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        do_sample=temperature > 0,\n",
    "        return_full_text=False,\n",
    "    ).strip()\n",
    "\n",
    "def call_model(spec: ModelSpec, text: str, temperature: float=0.0, max_tokens:int=3) -> str:\n",
    "    prompt = make_inst_prompt(text)\n",
    "\n",
    "    if spec.provider == \"hf_api\":\n",
    "        mid = spec.model_id.lower()\n",
    "        if \"llama-2-13b\" in mid:\n",
    "            client = hf_client_en\n",
    "        elif \"rebatch/llama-3-8b-dutch\" in mid:\n",
    "            client = hf_client_8b_dutch\n",
    "        elif \"meta-llama/Meta-Llama-3-8B\".lower() in mid or (\"llama-3\" in mid and \"8b\" in mid):\n",
    "            client = hf_client_8b_en\n",
    "        else:\n",
    "            return \"[error_hf:client_map] Onbekend HF model_id voor deze run\"\n",
    "\n",
    "        try:\n",
    "            return _hf_text_gen(client, prompt, temperature, max_tokens)\n",
    "        except Exception as e:\n",
    "            msg = str(e)\n",
    "            if \"Supported task: conversational\" in msg or \"not supported for task text-generation\" in msg:\n",
    "                try:\n",
    "                    return _hf_chat_completion(client, prompt, temperature, max_tokens)\n",
    "                except Exception as e2:\n",
    "                    return f\"[error_hf_chat:{type(e2).__name__}] {e2}\"\n",
    "            return f\"[error_hf:{type(e).__name__}] {e}\"\n",
    "\n",
    "    if spec.provider == \"groq\":\n",
    "        # 70B via GROQ\n",
    "        resp = groq_client.chat.completions.create(\n",
    "            model=spec.model_id,              # bijv. \"llama-3.3-70b-versatile\"\n",
    "            messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        return resp.choices[0].message.content.strip()\n",
    "\n",
    "    if spec.provider == \"openai\":\n",
    "        assert openai_client, \"OPENAI_API_KEY ontbreekt in .env\"\n",
    "        resp = openai_client.chat.completions.create(\n",
    "            model=spec.model_id,              # bijv. \"gpt-5\"\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_completion_tokens=max_tokens,\n",
    "        )\n",
    "        return resp.choices[0].message.content.strip()\n",
    "\n",
    "    raise ValueError(f\"Unknown provider: {spec.provider}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f058b4d-5c5e-4b2a-a370-c65abf5f886b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModelSpec(name='GPT-5', provider='openai', model_id='gpt-5', language='nl'),\n",
       " ModelSpec(name='HF-Llama-3-8B-dutch', provider='hf_api', model_id='ReBatch/Llama-3-8B-dutch', language='nl'),\n",
       " ModelSpec(name='HF-Llama-3-8B-en', provider='hf_api', model_id='meta-llama/Meta-Llama-3-8B', language='nl'),\n",
       " ModelSpec(name='Groq-Llama-70B', provider='groq', model_id='llama-3.3-70b-versatile', language='nl')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODELS = [\n",
    "    # Llama-2-13B (EN) via jouw endpoint\n",
    "    #ModelSpec(\n",
    "     #   name=\"HF-Llama-2-13B-en\",\n",
    "      #  provider=\"hf_api\",\n",
    "       # model_id=\"meta-llama/Llama-2-13b-chat-hf\",\n",
    "        #language=\"nl\",\n",
    "    #),\n",
    "    # OpenAI — GPT-5 (zorg dat je key & toegang hebt)\n",
    "    ModelSpec(\n",
    "        name=\"GPT-5\",\n",
    "        provider=\"openai\",\n",
    "        model_id=\"gpt-5\",   # gebruik het exacte model-id dat in jouw account beschikbaar is\n",
    "        language=\"nl\",\n",
    "    ),\n",
    "    # ReBatch Llama-3-8B Dutch via jouw endpoint\n",
    "    ModelSpec(\n",
    "        name=\"HF-Llama-3-8B-dutch\",\n",
    "        provider=\"hf_api\",\n",
    "        model_id=\"ReBatch/Llama-3-8B-dutch\",\n",
    "        language=\"nl\",\n",
    "    ),\n",
    "    # Meta-Llama-3-8B (EN) via jouw endpoint\n",
    "    ModelSpec(\n",
    "        name=\"HF-Llama-3-8B-en\",\n",
    "        provider=\"hf_api\",\n",
    "        model_id=\"meta-llama/Meta-Llama-3-8B\",\n",
    "        language=\"nl\",\n",
    "    ),\n",
    "    # Llama 70B via GROQ\n",
    "    ModelSpec(\n",
    "        name=\"Groq-Llama-70B\",\n",
    "        provider=\"groq\",\n",
    "        model_id=\"llama-3.3-70b-versatile\", \n",
    "        language=\"nl\",\n",
    "    )\n",
    "]\n",
    "\n",
    "MODELS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ceedfcb0-7c0f-43b7-8311-7757f320c4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Running GPT-5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [03:26<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Running HF-Llama-3-8B-dutch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 70/70 [00:00<00:00, 187.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Running HF-Llama-3-8B-en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 70/70 [00:00<00:00, 166.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Running Groq-Llama-70B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [01:05<00:00,  1.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title_text</th>\n",
       "      <th>lead_text</th>\n",
       "      <th>body_text</th>\n",
       "      <th>GPT_5_title_label</th>\n",
       "      <th>GPT_5_title_score</th>\n",
       "      <th>GPT_5_lead_label</th>\n",
       "      <th>GPT_5_lead_score</th>\n",
       "      <th>GPT_5_body_label</th>\n",
       "      <th>GPT_5_body_score</th>\n",
       "      <th>...</th>\n",
       "      <th>HF_Llama_3_8B_en_lead_label</th>\n",
       "      <th>HF_Llama_3_8B_en_lead_score</th>\n",
       "      <th>HF_Llama_3_8B_en_body_label</th>\n",
       "      <th>HF_Llama_3_8B_en_body_score</th>\n",
       "      <th>Groq_Llama_70B_title_label</th>\n",
       "      <th>Groq_Llama_70B_title_score</th>\n",
       "      <th>Groq_Llama_70B_lead_label</th>\n",
       "      <th>Groq_Llama_70B_lead_score</th>\n",
       "      <th>Groq_Llama_70B_body_label</th>\n",
       "      <th>Groq_Llama_70B_body_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Nederlandse patiënt wacht te lang op betere me...</td>\n",
       "      <td>Wat een prachtig bericht onlangs, dat meer kan...</td>\n",
       "      <td>Maar het is jammer dat het zo lang duurt voord...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Nieuwe kankermedicijnen leveren meer financiël...</td>\n",
       "      <td>Vorige week verscheen in Trouw een artikel met...</td>\n",
       "      <td>Nederland is een mooi land waarin uiteindelijk...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>Hoe controleer je verstopte moedervlekken?</td>\n",
       "      <td>Meer dan twintig jaar geleden ontdekte ze op h...</td>\n",
       "      <td>Eerst over die dagelijkse inspectie. Dat is ec...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>'Ik vind het erg als 'n infuus van 25.000 euro...</td>\n",
       "      <td>Waarom schrijven artsen 1005 milligram van een...</td>\n",
       "      <td>Ziekenhuisapotheker Roelof van Leeuwen zet zic...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>Wachtlijsten en personeelstekort: het 'zorginf...</td>\n",
       "      <td>De gezondheidszorg is 'op', er zit geen rek me...</td>\n",
       "      <td>Verpleegkundigen, verzorgenden, huisartsen, sp...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                         title_text  \\\n",
       "0   7  Nederlandse patiënt wacht te lang op betere me...   \n",
       "1  10  Nieuwe kankermedicijnen leveren meer financiël...   \n",
       "2  11         Hoe controleer je verstopte moedervlekken?   \n",
       "3  16  'Ik vind het erg als 'n infuus van 25.000 euro...   \n",
       "4  21  Wachtlijsten en personeelstekort: het 'zorginf...   \n",
       "\n",
       "                                           lead_text  \\\n",
       "0  Wat een prachtig bericht onlangs, dat meer kan...   \n",
       "1  Vorige week verscheen in Trouw een artikel met...   \n",
       "2  Meer dan twintig jaar geleden ontdekte ze op h...   \n",
       "3  Waarom schrijven artsen 1005 milligram van een...   \n",
       "4  De gezondheidszorg is 'op', er zit geen rek me...   \n",
       "\n",
       "                                           body_text GPT_5_title_label  \\\n",
       "0  Maar het is jammer dat het zo lang duurt voord...              None   \n",
       "1  Nederland is een mooi land waarin uiteindelijk...              None   \n",
       "2  Eerst over die dagelijkse inspectie. Dat is ec...              None   \n",
       "3  Ziekenhuisapotheker Roelof van Leeuwen zet zic...              None   \n",
       "4  Verpleegkundigen, verzorgenden, huisartsen, sp...              None   \n",
       "\n",
       "  GPT_5_title_score GPT_5_lead_label GPT_5_lead_score GPT_5_body_label  \\\n",
       "0              None             None             None             None   \n",
       "1              None             None             None             None   \n",
       "2              None             None             None             None   \n",
       "3              None             None             None             None   \n",
       "4              None             None             None             None   \n",
       "\n",
       "  GPT_5_body_score  ... HF_Llama_3_8B_en_lead_label  \\\n",
       "0             None  ...                        None   \n",
       "1             None  ...                        None   \n",
       "2             None  ...                        None   \n",
       "3             None  ...                        None   \n",
       "4             None  ...                        None   \n",
       "\n",
       "  HF_Llama_3_8B_en_lead_score HF_Llama_3_8B_en_body_label  \\\n",
       "0                        None                        None   \n",
       "1                        None                        None   \n",
       "2                        None                        None   \n",
       "3                        None                        None   \n",
       "4                        None                        None   \n",
       "\n",
       "  HF_Llama_3_8B_en_body_score Groq_Llama_70B_title_label  \\\n",
       "0                        None                   negative   \n",
       "1                        None                   negative   \n",
       "2                        None                   positive   \n",
       "3                        None                   negative   \n",
       "4                        None                   negative   \n",
       "\n",
       "  Groq_Llama_70B_title_score Groq_Llama_70B_lead_label  \\\n",
       "0                         -1                  positive   \n",
       "1                         -1                  negative   \n",
       "2                          1                  negative   \n",
       "3                         -1                  negative   \n",
       "4                         -1                  negative   \n",
       "\n",
       "  Groq_Llama_70B_lead_score Groq_Llama_70B_body_label  \\\n",
       "0                         1                  negative   \n",
       "1                        -1                  negative   \n",
       "2                        -1                  positive   \n",
       "3                        -1                  positive   \n",
       "4                        -1                  negative   \n",
       "\n",
       "  Groq_Llama_70B_body_score  \n",
       "0                        -1  \n",
       "1                        -1  \n",
       "2                         1  \n",
       "3                         1  \n",
       "4                        -1  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%python\n",
    "results = df[[\"id\",\"title_text\",\"lead_text\",\"body_text\"]].copy()\n",
    "\n",
    "for spec in MODELS:\n",
    "    print(f\"==> Running {spec.name}\")\n",
    "    base = safe_colname(spec.name)\n",
    "    tlab, tscore = [], []\n",
    "    llab, lscore = [], []\n",
    "    blab, bscore = [], []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        r_title = classify_text(spec, row[\"title_text\"])\n",
    "        r_lead  = classify_text(spec, row[\"lead_text\"])\n",
    "        r_body  = classify_text(spec, row[\"body_text\"])\n",
    "\n",
    "        tlab.append(r_title[\"label\"]); tscore.append(r_title[\"score\"])\n",
    "        llab.append(r_lead[\"label\"]);  lscore.append(r_lead[\"score\"])\n",
    "        blab.append(r_body[\"label\"]);  bscore.append(r_body[\"score\"])\n",
    "\n",
    "    results[f\"{base}_title_label\"] = tlab\n",
    "    results[f\"{base}_title_score\"] = tscore\n",
    "    results[f\"{base}_lead_label\"]  = llab\n",
    "    results[f\"{base}_lead_score\"]  = lscore\n",
    "    results[f\"{base}_body_label\"]  = blab\n",
    "    results[f\"{base}_body_score\"]  = bscore\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b707309e-15a5-4eba-b54f-0e725a225b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup geschreven → out\\newspapers_sentiment_llms.backup.xlsx\n",
      "Geüpdatet → out\\newspapers_sentiment_llms.xlsx\n",
      "Let op: geen *_overall_label kolommen gevonden om te tonen (controleer namen).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "IN_PATH  = Path(\"out/newspapers_sentiment_llms.xlsx\")\n",
    "BACKUP   = IN_PATH.with_suffix(\".backup.xlsx\")\n",
    "\n",
    "if not IN_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Bestand niet gevonden: {IN_PATH} (pas pad/naam aan indien nodig)\")\n",
    "\n",
    "# 1) Lees bestaande output\n",
    "df_out = pd.read_excel(IN_PATH)\n",
    "\n",
    "# 2) Bepaal welke modellen we nu willen updaten (alleen 13B EN & 70B in je huidige MODELS)\n",
    "#    We pakken exact de kolommen die in 'results' voor die modellen zijn aangemaakt.\n",
    "model_bases = [safe_colname(m.name) for m in MODELS]  # bv. ['HF_Llama_2_13B_en','HF_Llama_3_70B']\n",
    "cols_to_copy = [\"id\"]\n",
    "for base in model_bases:\n",
    "    # alle kolommen in 'results' die met dit base beginnen\n",
    "    cols_for_base = [c for c in results.columns if c.startswith(base + \"_\")]\n",
    "    cols_to_copy.extend(cols_for_base)\n",
    "\n",
    "# 3) Maak nieuw subset uit 'results' met alleen de relevante kolommen\n",
    "new_sub = results.loc[:, [c for c in cols_to_copy if c in results.columns]].copy()\n",
    "\n",
    "# 4) Overwrite per kolom op basis van 'id'\n",
    "#    We gebruiken map()+combine_first zodat nieuwe waarden voorrang krijgen,\n",
    "#    en oude waarden behouden blijven waar geen nieuwe zijn.\n",
    "if \"id\" not in df_out.columns:\n",
    "    raise ValueError(\"Kolom 'id' ontbreekt in bestaande output. Kan niet mergen.\")\n",
    "\n",
    "new_sub_indexed = new_sub.set_index(\"id\")\n",
    "\n",
    "for col in new_sub.columns:\n",
    "    if col == \"id\":\n",
    "        continue\n",
    "    # mapping van id -> nieuwe waarde\n",
    "    mapping = new_sub_indexed[col]\n",
    "    # projecteer nieuwe waarden op df_out in 'm'\n",
    "    m = df_out[\"id\"].map(mapping)\n",
    "    if col in df_out.columns:\n",
    "        # nieuwe waardes overschrijven waar beschikbaar, anders bestaande behouden\n",
    "        df_out[col] = m.combine_first(df_out[col])\n",
    "    else:\n",
    "        # kolom bestond nog niet: direct toevoegen\n",
    "        df_out[col] = m\n",
    "\n",
    "# 5) Backup maken en terugschrijven\n",
    "if not BACKUP.exists():\n",
    "    df_out.to_excel(BACKUP, index=False)\n",
    "    print(f\"Backup geschreven → {BACKUP}\")\n",
    "\n",
    "# Schrijf naar hetzelfde bestand\n",
    "df_out.to_excel(IN_PATH, index=False)\n",
    "print(f\"Geüpdatet → {IN_PATH}\")\n",
    "\n",
    "# 6) (optioneel) Snelle controle: toon de net geüpdatete overall labels\n",
    "check_cols = []\n",
    "for base in model_bases:\n",
    "    colname = f\"{base}_overall_label\"\n",
    "    if colname in df_out.columns:\n",
    "        check_cols.append(colname)\n",
    "\n",
    "if check_cols:\n",
    "    display(df_out[[\"id\"] + check_cols].head())\n",
    "else:\n",
    "    print(\"Let op: geen *_overall_label kolommen gevonden om te tonen (controleer namen).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8dc981c3-5c24-4f01-8398-37882a501808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maak overall_label voor: ['GPT_5', 'Groq_Llama_70B', 'HF_Llama_3_8B_dutch', 'HF_Llama_3_8B_en']\n",
      "Geüpdatet → out\\newspapers_sentiment_llms.xlsx\n",
      "\n",
      "=== HF_Llama_2_13B_dutch_overall_label ===\n",
      "HF_Llama_2_13B_dutch_overall_label\n",
      "positive    70\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== GPT_4_0_overall_label ===\n",
      "GPT_4_0_overall_label\n",
      "negative    40\n",
      "positive    30\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Groq_Llama_70B_overall_label ===\n",
      "Groq_Llama_70B_overall_label\n",
      "positive    36\n",
      "negative    34\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# %%python\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "IN_PATH = Path(\"out/newspapers_sentiment_llms.xlsx\")\n",
    "BACKUP  = IN_PATH.with_suffix(\".backup.xlsx\")\n",
    "\n",
    "df = pd.read_excel(IN_PATH)\n",
    "\n",
    "# 1) Vind alle \"bases\" die score-kolommen hebben\n",
    "def collect_bases_with_scores(columns):\n",
    "    bases = set()\n",
    "    for c in columns:\n",
    "        if c.endswith(\"_title_score\") or c.endswith(\"_lead_score\") or c.endswith(\"_body_score\"):\n",
    "            base = c.rsplit(\"_\", 2)[0]  # strip \"_{part}_score\"\n",
    "            bases.add(base)\n",
    "    return sorted(bases)\n",
    "\n",
    "# 2) Check welke bases géén overall_label hebben\n",
    "def bases_missing_overall_label(df, bases):\n",
    "    missing = []\n",
    "    for base in bases:\n",
    "        if f\"{base}_overall_label\" not in df.columns:\n",
    "            # Alleen meenemen als er minstens één scorekolom bestaat\n",
    "            parts = [f\"{base}_title_score\", f\"{base}_lead_score\", f\"{base}_body_score\"]\n",
    "            if any(p in df.columns for p in parts):\n",
    "                missing.append(base)\n",
    "    return missing\n",
    "\n",
    "def infer_label_from_scores(row, base):\n",
    "    parts = []\n",
    "    for p in (\"title\", \"lead\", \"body\"):\n",
    "        col = f\"{base}_{p}_score\"\n",
    "        if col in row and pd.notna(row[col]):\n",
    "            parts.append(row[col])\n",
    "\n",
    "    if not parts:\n",
    "        return np.nan  # geen info\n",
    "\n",
    "    arr = np.array(parts, dtype=float)\n",
    "\n",
    "    # Heuristiek: als alle scores binnen [0,1] liggen -> threshold 0.5\n",
    "    # anders ga uit van [-1,1] en threshold 0.0\n",
    "    finite = arr[np.isfinite(arr)]\n",
    "    if finite.size == 0:\n",
    "        return np.nan\n",
    "\n",
    "    scores_min, scores_max = finite.min(), finite.max()\n",
    "    avg = float(finite.mean())\n",
    "\n",
    "    if 0.0 <= scores_min and scores_max <= 1.0:\n",
    "        # binaire score (0/1 of prob)\n",
    "        return \"positive\" if avg >= 0.5 else \"negative\"\n",
    "    else:\n",
    "        # signed score (-1..1)\n",
    "        if avg > 0.0:\n",
    "            return \"positive\"\n",
    "        elif avg < 0.0:\n",
    "            return \"negative\"\n",
    "        else:\n",
    "            # exact gelijk -> kies 'negative' als conservatieve default\n",
    "            return \"negative\"\n",
    "\n",
    "# 3) Bereken ontbrekende overall_labels (o.a. Groq_Llama_70B)\n",
    "bases = collect_bases_with_scores(df.columns)\n",
    "missing = bases_missing_overall_label(df, bases)\n",
    "\n",
    "if not missing:\n",
    "    print(\"Geen bases gevonden met ontbrekende *_overall_label op basis van *_score kolommen.\")\n",
    "else:\n",
    "    print(\"Maak overall_label voor:\", missing)\n",
    "    for base in missing:\n",
    "        df[f\"{base}_overall_label\"] = df.apply(lambda r: infer_label_from_scores(r, base), axis=1)\n",
    "\n",
    "    # Backup (eenmalig)\n",
    "    if not BACKUP.exists():\n",
    "        df.to_excel(BACKUP, index=False)\n",
    "        print(f\"Backup geschreven → {BACKUP}\")\n",
    "\n",
    "    # Schrijf terug\n",
    "    df.to_excel(IN_PATH, index=False)\n",
    "    print(f\"Geüpdatet → {IN_PATH}\")\n",
    "\n",
    "# 4) Samenvatting: verdeling voor kolommen zonder NaN\n",
    "overall_cols = [c for c in df.columns if c.endswith(\"_overall_label\")]\n",
    "non_na_cols = [c for c in overall_cols if df[c].notna().all()]\n",
    "\n",
    "if not non_na_cols:\n",
    "    print(\"Geen *_overall_label kolommen zonder NaN gevonden.\")\n",
    "else:\n",
    "    for col in non_na_cols:\n",
    "        print(f\"\\n=== {col} ===\")\n",
    "        print(df[col].value_counts(dropna=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
